{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspiration for Neural Net: https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f575571edf0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pitcher\n",
    "from random import shuffle\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "player = 'verlander'\n",
    "data = pd.read_csv('Data/raw_data/'+player+'.csv')\n",
    "data = pitcher.clean_data(data)\n",
    "ABs = pitcher.get_abs(data)\n",
    "reps = pitcher.get_reps(ABs)\n",
    "reps = pitcher.drop_nas(reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get train, validate, and test sets (60-20-20)\n",
    "shuffle(reps)\n",
    "cutoff1 = int(len(reps)*0.6)\n",
    "cutoff2 = cutoff1 + int(len(reps)*.2)\n",
    "train = reps[1:cutoff1]\n",
    "validate = reps[cutoff1:cutoff2]\n",
    "test = reps[cutoff2:]\n",
    "\n",
    "\n",
    "#get train, validate, and test batches\n",
    "batch_size = 50\n",
    "train_batches = pitcher.get_batches(train,batch_size)\n",
    "test_batches = pitcher.get_batches(test,batch_size)\n",
    "validate_batches = pitcher.get_batches(validate,batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(batches,model):\n",
    "    length = len(batches)*len(batches[0][0])\n",
    "    num_right = 0\n",
    "    ch_count = 0\n",
    "    predict_ch = 0\n",
    "    cu_count = 0\n",
    "    predict_cu = 0\n",
    "    sl_count = 0\n",
    "    predict_sl = 0\n",
    "    ff_count = 0\n",
    "    predict_ff = 0\n",
    "    ff_right = 0\n",
    "    ch_right = 0\n",
    "    sl_right = 0\n",
    "    cu_right = 0\n",
    "    for batch in batches:\n",
    "        with torch.no_grad():\n",
    "            prev_pitches,pre_pitch, ptypes = batch\n",
    "            prevs_in = torch.tensor(prev_pitches, dtype=torch.float)\n",
    "            rep_in = torch.tensor(pre_pitch, dtype=torch.float)\n",
    "            #targets = tag_to_ix[ptypes]\n",
    "            if len(ptypes) > 1:\n",
    "                targets = [tag_to_ix[ptype] for ptype in ptypes]\n",
    "            else:\n",
    "                targets = tag_to_ix[ptypes[0]]\n",
    "            tag_scores = model((prevs_in,rep_in))\n",
    "            #print(tag_scores)\n",
    "            preds = [tag_score.max(0) for tag_score in tag_scores]\n",
    "            #print(index.item())\n",
    "            if len(ptypes) > 1:\n",
    "                for pred,target in zip(preds,targets):\n",
    "                    _,index = pred\n",
    "                    if target == 1:\n",
    "                        ff_count += 1\n",
    "                    if index.item() == 1:\n",
    "                        predict_ff += 1\n",
    "                    if target == 1 and index.item() == 1:\n",
    "                        ff_right += 1\n",
    "                    if target == 0:\n",
    "                        cu_count += 1\n",
    "                    if index.item() == 0:\n",
    "                        predict_cu += 1\n",
    "                    if target == 0 and index.item() == 0:\n",
    "                        cu_right += 1\n",
    "                    if index.item() == target:\n",
    "                        num_right += 1\n",
    "            else:\n",
    "                _,index = preds[0]\n",
    "                if targets == 1:\n",
    "                    ff_count += 1\n",
    "                if index.item() == 1:\n",
    "                    predict_ff += 1\n",
    "                if targets == 1 and index.item() == 1:\n",
    "                    ff_right += 1\n",
    "                if targets == 0:\n",
    "                    cu_count += 1\n",
    "                if index.item() == 0:\n",
    "                    predict_cu += 1\n",
    "                if targets == 0 and index.item() == 0:\n",
    "                    cu_right += 1\n",
    "                if index.item() == targets:\n",
    "                    num_right += 1\n",
    "    ff_rate = ff_count/length\n",
    "    pred_ff_rate = predict_ff/length\n",
    "    ff_acc = ff_right/ff_count\n",
    "    cu_rate = cu_count/length\n",
    "    pred_cu_rate = predict_cu/length\n",
    "    cu_acc = cu_right/cu_count\n",
    "    accuracy = num_right/length\n",
    "    print(\"______________________________________\")\n",
    "    print(\"Non-Fastball rate:\",cu_rate)\n",
    "    print(\"Predicted Non-Fastball rate:\",pred_cu_rate)\n",
    "    print(\"Non-Fastball accuracy:\",cu_acc)\n",
    "    print(\"______________________________________\")\n",
    "    print(\"Fastball rate:\",ff_rate)\n",
    "    print(\"Predicted Fastball rate:\",pred_ff_rate)\n",
    "    print(\"Fastball accuracy:\",ff_acc)\n",
    "    print(\"______________________________________\")\n",
    "    print(\"Accuracy:\",accuracy)\n",
    "    print(\"Accuracy above naive guess:\",accuracy - ff_rate)\n",
    "    print(\"______________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change this for different pitcher\n",
    "'''tag_to_ix = {'CU':0,\n",
    "             'FF':1,\n",
    "             'SL':2,\n",
    "             'CH':3}'''\n",
    "tag_to_ix = {'NF':0,\n",
    "             'FF':1}\n",
    "\n",
    "# -- Input Dimensions -- DO NOT CHANGE\n",
    "PREV_PITCH_DIM = 25\n",
    "NUM_PREV_PITCHES = 3\n",
    "GAME_STATE_DIM = 15 \n",
    "GAME_OUT_DIM = 15\n",
    "\n",
    "# -- Hyperparameters -- DO CHANGE \n",
    "HIDDEN_DIM = 120\n",
    "OUT_DIM = 15\n",
    "lstm_layers = 1\n",
    "learning_rate = 0.0001\n",
    "\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "    \n",
    "class PitchPredict(nn.Module):\n",
    "    def __init__(self, prev_pitch_dim, hidden_dim, num_prev_pitches,out_dim, game_state_dim, game_out_dim, num_ptypes):\n",
    "        super(PitchPredict, self).__init__()\n",
    "        #get constants\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.prev_pitch_dim = prev_pitch_dim\n",
    "        self.num_prev_pitches = num_prev_pitches\n",
    "        self.out_dim = out_dim\n",
    "        self.game_state_dim = game_state_dim\n",
    "        self.game_out_dim = game_out_dim\n",
    "        \n",
    "        ####Define Layers####\n",
    "        \n",
    "        ########################## LSTM for past five pitches#######################################\n",
    "        # The LSTM takes previous pitch vectors as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.RNN(prev_pitch_dim, hidden_dim,num_layers=lstm_layers,batch_first=True)#FIDDLE WITH num_layers\n",
    "\n",
    "        # The linear layer that maps from hidden state space to a vector\n",
    "        # with dimensionality OUT_DIM\n",
    "        self.hidden2out = nn.Linear(hidden_dim, out_dim)\n",
    "        \n",
    "        ############## FULLY CONNECTED LAYERS for LSTM OUTPUT + game_state vector #################\n",
    "        \n",
    "        # layer to map the game state vector to a different dimension\n",
    "        # self.l1 = nn.Linear(self.game_state_dim, self.game_out_dim)  \n",
    "        \n",
    "        # This Fully connected layer maps from the output of the final hidden layer output from the LSTM,\n",
    "        # dimension = OUT_DIM, with the game state vector, dimension = GAME_STATE_DIM\n",
    "        # to a vector of length of the number of ptypes to pass through softmax for probabilities\n",
    "        self.fc1 = nn.Linear((self.out_dim + self.game_out_dim), 2)\n",
    "\n",
    "                             \n",
    "    def forward(self, rep):\n",
    "        past_pitches,game_state = rep\n",
    "        lstm_out, _ = self.lstm(past_pitches.view(batch_size,self.num_prev_pitches, -1))\n",
    "        learned_rep = self.hidden2out(lstm_out.view(batch_size,self.num_prev_pitches, -1))\n",
    "        game_rep = game_state.view(batch_size,self.game_state_dim)\n",
    "        #game_rep = self.l1(game_state.view(batch_size,self.game_state_dim))\n",
    "        #game_rep = F.relu(game_rep)\n",
    "        encoding = learned_rep[:,self.num_prev_pitches - 1:,:]\n",
    "        fc_in = torch.cat((encoding.view(batch_size,self.out_dim),game_rep.view(batch_size,self.game_out_dim)),dim=1)\n",
    "        fc = self.fc1(fc_in.view(batch_size,self.game_out_dim+self.out_dim))\n",
    "        tag_scores = F.log_softmax(fc,dim=0)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************Pre-Training Accuracy*****************\n",
      "______________________________________\n",
      "Non-Fastball rate: 0.42080338266384776\n",
      "Predicted Non-Fastball rate: 0.5016490486257928\n",
      "Non-Fastball accuracy: 0.46694131832797425\n",
      "______________________________________\n",
      "Fastball rate: 0.5791966173361522\n",
      "Predicted Fastball rate: 0.4983509513742072\n",
      "Fastball accuracy: 0.4731347641991532\n",
      "______________________________________\n",
      "Accuracy: 0.4705285412262156\n",
      "Accuracy above naive guess: -0.10866807610993662\n",
      "______________________________________\n",
      "**************************Training*****************************\n",
      "epoch: 1 loss: 3.9207334518432617\n",
      "epoch: 2 loss: 3.9105803966522217\n",
      "epoch: 3 loss: 3.9061243534088135\n",
      "epoch: 4 loss: 3.902644395828247\n",
      "epoch: 5 loss: 3.8998770713806152\n",
      "epoch: 6 loss: 3.897428512573242\n",
      "epoch: 7 loss: 3.895267963409424\n",
      "epoch: 8 loss: 3.8934314250946045\n",
      "epoch: 9 loss: 3.8919076919555664\n",
      "epoch: 10 loss: 3.8906466960906982\n",
      "epoch: 11 loss: 3.8896055221557617\n",
      "epoch: 12 loss: 3.8887338638305664\n",
      "epoch: 13 loss: 3.8879852294921875\n",
      "epoch: 14 loss: 3.8873345851898193\n",
      "epoch: 15 loss: 3.8867855072021484\n",
      "epoch: 16 loss: 3.8863205909729004\n",
      "epoch: 17 loss: 3.8859145641326904\n",
      "epoch: 18 loss: 3.8855528831481934\n",
      "epoch: 19 loss: 3.885225772857666\n",
      "epoch: 20 loss: 3.8849222660064697\n",
      "epoch: 21 loss: 3.884631872177124\n",
      "epoch: 22 loss: 3.884345769882202\n",
      "epoch: 23 loss: 3.8840622901916504\n",
      "epoch: 24 loss: 3.8837897777557373\n",
      "epoch: 25 loss: 3.8835372924804688\n",
      "epoch: 26 loss: 3.883302688598633\n",
      "epoch: 27 loss: 3.883084774017334\n",
      "epoch: 28 loss: 3.8828787803649902\n",
      "epoch: 29 loss: 3.8826792240142822\n",
      "epoch: 30 loss: 3.8824880123138428\n",
      "epoch: 31 loss: 3.8822991847991943\n",
      "epoch: 32 loss: 3.8821170330047607\n",
      "epoch: 33 loss: 3.8819386959075928\n",
      "epoch: 34 loss: 3.881765127182007\n",
      "epoch: 35 loss: 3.881593704223633\n",
      "epoch: 36 loss: 3.8814263343811035\n",
      "epoch: 37 loss: 3.881260871887207\n",
      "epoch: 38 loss: 3.881096601486206\n",
      "epoch: 39 loss: 3.880934953689575\n",
      "epoch: 40 loss: 3.8807740211486816\n",
      "epoch: 41 loss: 3.8806118965148926\n",
      "epoch: 42 loss: 3.8804492950439453\n",
      "epoch: 43 loss: 3.8802859783172607\n",
      "epoch: 44 loss: 3.880119562149048\n",
      "epoch: 45 loss: 3.8799517154693604\n",
      "epoch: 46 loss: 3.8797872066497803\n",
      "epoch: 47 loss: 3.8796234130859375\n",
      "epoch: 48 loss: 3.8794655799865723\n",
      "epoch: 49 loss: 3.8793108463287354\n",
      "epoch: 50 loss: 3.8791589736938477\n",
      "epoch: 51 loss: 3.879009962081909\n",
      "epoch: 52 loss: 3.8788628578186035\n",
      "epoch: 53 loss: 3.8787167072296143\n",
      "epoch: 54 loss: 3.8785712718963623\n",
      "epoch: 55 loss: 3.8784255981445312\n",
      "epoch: 56 loss: 3.878279209136963\n",
      "epoch: 57 loss: 3.878131628036499\n",
      "epoch: 58 loss: 3.877980947494507\n",
      "epoch: 59 loss: 3.8778293132781982\n",
      "epoch: 60 loss: 3.877673387527466\n",
      "epoch: 61 loss: 3.8775153160095215\n",
      "epoch: 62 loss: 3.8773550987243652\n",
      "epoch: 63 loss: 3.877190351486206\n",
      "epoch: 64 loss: 3.877021551132202\n",
      "epoch: 65 loss: 3.876851797103882\n",
      "epoch: 66 loss: 3.8766791820526123\n",
      "epoch: 67 loss: 3.8765041828155518\n",
      "epoch: 68 loss: 3.8763253688812256\n",
      "epoch: 69 loss: 3.876145601272583\n",
      "epoch: 70 loss: 3.875962257385254\n",
      "epoch: 71 loss: 3.8757760524749756\n",
      "epoch: 72 loss: 3.8755881786346436\n",
      "epoch: 73 loss: 3.8753957748413086\n",
      "epoch: 74 loss: 3.8752012252807617\n",
      "epoch: 75 loss: 3.8750030994415283\n",
      "epoch: 76 loss: 3.874804973602295\n",
      "epoch: 77 loss: 3.874601364135742\n",
      "epoch: 78 loss: 3.874396562576294\n",
      "epoch: 79 loss: 3.8741891384124756\n",
      "epoch: 80 loss: 3.8739805221557617\n",
      "epoch: 81 loss: 3.8737685680389404\n",
      "epoch: 82 loss: 3.873555898666382\n",
      "epoch: 83 loss: 3.8733410835266113\n",
      "epoch: 84 loss: 3.8731250762939453\n",
      "epoch: 85 loss: 3.872908115386963\n",
      "epoch: 86 loss: 3.872689723968506\n",
      "epoch: 87 loss: 3.872469425201416\n",
      "epoch: 88 loss: 3.8722493648529053\n",
      "epoch: 89 loss: 3.8720288276672363\n",
      "epoch: 90 loss: 3.871807336807251\n",
      "epoch: 91 loss: 3.871586322784424\n",
      "epoch: 92 loss: 3.871366024017334\n",
      "epoch: 93 loss: 3.8711459636688232\n",
      "epoch: 94 loss: 3.870926856994629\n",
      "epoch: 95 loss: 3.8707077503204346\n",
      "epoch: 96 loss: 3.870488166809082\n",
      "epoch: 97 loss: 3.870266914367676\n",
      "epoch: 98 loss: 3.870041847229004\n",
      "epoch: 99 loss: 3.8698151111602783\n",
      "epoch: 100 loss: 3.869586229324341\n",
      "epoch: 101 loss: 3.8693575859069824\n",
      "epoch: 102 loss: 3.8691351413726807\n",
      "epoch: 103 loss: 3.8689181804656982\n",
      "epoch: 104 loss: 3.8687081336975098\n",
      "epoch: 105 loss: 3.8685033321380615\n",
      "epoch: 106 loss: 3.8683056831359863\n",
      "epoch: 107 loss: 3.868109941482544\n",
      "epoch: 108 loss: 3.867917537689209\n",
      "epoch: 109 loss: 3.8677291870117188\n",
      "epoch: 110 loss: 3.867541551589966\n",
      "epoch: 111 loss: 3.867356538772583\n",
      "epoch: 112 loss: 3.867173194885254\n",
      "epoch: 113 loss: 3.8669919967651367\n",
      "epoch: 114 loss: 3.866814613342285\n",
      "epoch: 115 loss: 3.8666374683380127\n",
      "epoch: 116 loss: 3.8664636611938477\n",
      "epoch: 117 loss: 3.8662919998168945\n",
      "epoch: 118 loss: 3.8661227226257324\n",
      "epoch: 119 loss: 3.8659560680389404\n",
      "epoch: 120 loss: 3.8657925128936768\n",
      "epoch: 121 loss: 3.8656322956085205\n",
      "epoch: 122 loss: 3.8654749393463135\n",
      "epoch: 123 loss: 3.865321636199951\n",
      "epoch: 124 loss: 3.865170955657959\n",
      "epoch: 125 loss: 3.8650240898132324\n",
      "epoch: 126 loss: 3.8648815155029297\n",
      "epoch: 127 loss: 3.8647425174713135\n",
      "epoch: 128 loss: 3.8646068572998047\n",
      "epoch: 129 loss: 3.864475727081299\n",
      "epoch: 130 loss: 3.8643484115600586\n",
      "epoch: 131 loss: 3.8642258644104004\n",
      "epoch: 132 loss: 3.864107131958008\n",
      "epoch: 133 loss: 3.863992691040039\n",
      "epoch: 134 loss: 3.8638839721679688\n",
      "epoch: 135 loss: 3.863778591156006\n",
      "epoch: 136 loss: 3.8636772632598877\n",
      "epoch: 137 loss: 3.8635811805725098\n",
      "epoch: 138 loss: 3.8634893894195557\n",
      "epoch: 139 loss: 3.8634021282196045\n",
      "epoch: 140 loss: 3.8633174896240234\n",
      "epoch: 141 loss: 3.86323881149292\n",
      "epoch: 142 loss: 3.8631632328033447\n",
      "epoch: 143 loss: 3.8630919456481934\n",
      "epoch: 144 loss: 3.863023042678833\n",
      "epoch: 145 loss: 3.8629586696624756\n",
      "epoch: 146 loss: 3.8628976345062256\n",
      "epoch: 147 loss: 3.86283802986145\n",
      "epoch: 148 loss: 3.8627817630767822\n",
      "epoch: 149 loss: 3.8627262115478516\n",
      "epoch: 150 loss: 3.862673282623291\n",
      "epoch: 151 loss: 3.862621545791626\n",
      "epoch: 152 loss: 3.862570285797119\n",
      "epoch: 153 loss: 3.862520694732666\n",
      "epoch: 154 loss: 3.862471342086792\n",
      "epoch: 155 loss: 3.862419366836548\n",
      "epoch: 156 loss: 3.862368106842041\n",
      "epoch: 157 loss: 3.862316370010376\n",
      "epoch: 158 loss: 3.8622617721557617\n",
      "epoch: 159 loss: 3.8622074127197266\n",
      "epoch: 160 loss: 3.862149953842163\n",
      "epoch: 161 loss: 3.8620903491973877\n",
      "epoch: 162 loss: 3.862028121948242\n",
      "epoch: 163 loss: 3.8619635105133057\n",
      "epoch: 164 loss: 3.86189603805542\n",
      "epoch: 165 loss: 3.8618249893188477\n",
      "epoch: 166 loss: 3.8617501258850098\n",
      "epoch: 167 loss: 3.8616738319396973\n",
      "epoch: 168 loss: 3.861593246459961\n",
      "epoch: 169 loss: 3.861509084701538\n",
      "epoch: 170 loss: 3.8614208698272705\n",
      "epoch: 171 loss: 3.861330032348633\n",
      "epoch: 172 loss: 3.8612332344055176\n",
      "epoch: 173 loss: 3.8611342906951904\n",
      "epoch: 174 loss: 3.8610315322875977\n",
      "epoch: 175 loss: 3.86092472076416\n",
      "epoch: 176 loss: 3.860814094543457\n",
      "epoch: 177 loss: 3.860701560974121\n",
      "epoch: 178 loss: 3.8605856895446777\n",
      "epoch: 179 loss: 3.860466957092285\n",
      "epoch: 180 loss: 3.8603451251983643\n",
      "epoch: 181 loss: 3.8602230548858643\n",
      "epoch: 182 loss: 3.8600997924804688\n",
      "epoch: 183 loss: 3.8599746227264404\n",
      "epoch: 184 loss: 3.859849214553833\n",
      "epoch: 185 loss: 3.859722375869751\n",
      "epoch: 186 loss: 3.8595950603485107\n",
      "epoch: 187 loss: 3.859468936920166\n",
      "epoch: 188 loss: 3.8593413829803467\n",
      "epoch: 189 loss: 3.8592140674591064\n",
      "epoch: 190 loss: 3.8590869903564453\n",
      "epoch: 191 loss: 3.8589601516723633\n",
      "epoch: 192 loss: 3.8588342666625977\n",
      "epoch: 193 loss: 3.8587093353271484\n",
      "epoch: 194 loss: 3.858583927154541\n",
      "epoch: 195 loss: 3.85845947265625\n",
      "epoch: 196 loss: 3.858335494995117\n",
      "epoch: 197 loss: 3.8582122325897217\n",
      "epoch: 198 loss: 3.858085870742798\n",
      "epoch: 199 loss: 3.857959508895874\n",
      "epoch: 200 loss: 3.8578319549560547\n",
      "epoch: 201 loss: 3.8577027320861816\n",
      "epoch: 202 loss: 3.857571601867676\n",
      "epoch: 203 loss: 3.8574411869049072\n",
      "epoch: 204 loss: 3.8573110103607178\n",
      "epoch: 205 loss: 3.857179641723633\n",
      "epoch: 206 loss: 3.8570499420166016\n",
      "epoch: 207 loss: 3.856921911239624\n",
      "epoch: 208 loss: 3.8567943572998047\n",
      "epoch: 209 loss: 3.856668472290039\n",
      "epoch: 210 loss: 3.8565454483032227\n",
      "epoch: 211 loss: 3.856423854827881\n",
      "epoch: 212 loss: 3.85630464553833\n",
      "epoch: 213 loss: 3.856187105178833\n",
      "epoch: 214 loss: 3.8560712337493896\n",
      "epoch: 215 loss: 3.8559577465057373\n",
      "epoch: 216 loss: 3.8558456897735596\n",
      "epoch: 217 loss: 3.8557345867156982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 218 loss: 3.855623483657837\n",
      "epoch: 219 loss: 3.85551381111145\n",
      "epoch: 220 loss: 3.8554046154022217\n",
      "epoch: 221 loss: 3.8552968502044678\n",
      "epoch: 222 loss: 3.855191230773926\n",
      "epoch: 223 loss: 3.8550853729248047\n",
      "epoch: 224 loss: 3.8549814224243164\n",
      "epoch: 225 loss: 3.8548784255981445\n",
      "epoch: 226 loss: 3.8547773361206055\n",
      "epoch: 227 loss: 3.8546767234802246\n",
      "epoch: 228 loss: 3.854578971862793\n",
      "epoch: 229 loss: 3.8544821739196777\n",
      "epoch: 230 loss: 3.8543875217437744\n",
      "epoch: 231 loss: 3.854295015335083\n",
      "epoch: 232 loss: 3.854203224182129\n",
      "epoch: 233 loss: 3.854114294052124\n",
      "epoch: 234 loss: 3.8540260791778564\n",
      "epoch: 235 loss: 3.853940486907959\n",
      "epoch: 236 loss: 3.853855609893799\n",
      "epoch: 237 loss: 3.8537728786468506\n",
      "epoch: 238 loss: 3.853691339492798\n",
      "epoch: 239 loss: 3.8536124229431152\n",
      "epoch: 240 loss: 3.853534460067749\n",
      "epoch: 241 loss: 3.8534581661224365\n",
      "epoch: 242 loss: 3.853383779525757\n",
      "epoch: 243 loss: 3.853311538696289\n",
      "epoch: 244 loss: 3.8532397747039795\n",
      "epoch: 245 loss: 3.8531689643859863\n",
      "epoch: 246 loss: 3.8531012535095215\n",
      "epoch: 247 loss: 3.8530349731445312\n",
      "epoch: 248 loss: 3.8529703617095947\n",
      "epoch: 249 loss: 3.8529062271118164\n",
      "epoch: 250 loss: 3.8528451919555664\n",
      "epoch: 251 loss: 3.852783441543579\n",
      "epoch: 252 loss: 3.8527252674102783\n",
      "epoch: 253 loss: 3.852668523788452\n",
      "epoch: 254 loss: 3.852612018585205\n",
      "epoch: 255 loss: 3.852557897567749\n",
      "epoch: 256 loss: 3.8525052070617676\n",
      "epoch: 257 loss: 3.852451801300049\n",
      "epoch: 258 loss: 3.852400302886963\n",
      "epoch: 259 loss: 3.8523519039154053\n",
      "epoch: 260 loss: 3.852302312850952\n",
      "epoch: 261 loss: 3.8522543907165527\n",
      "epoch: 262 loss: 3.8522086143493652\n",
      "epoch: 263 loss: 3.852161169052124\n",
      "epoch: 264 loss: 3.8521158695220947\n",
      "epoch: 265 loss: 3.8520708084106445\n",
      "epoch: 266 loss: 3.852024793624878\n",
      "epoch: 267 loss: 3.851979970932007\n",
      "epoch: 268 loss: 3.851935386657715\n",
      "epoch: 269 loss: 3.8518896102905273\n",
      "epoch: 270 loss: 3.8518435955047607\n",
      "epoch: 271 loss: 3.851797580718994\n",
      "epoch: 272 loss: 3.8517508506774902\n",
      "epoch: 273 loss: 3.851703405380249\n",
      "epoch: 274 loss: 3.8516547679901123\n",
      "epoch: 275 loss: 3.8516054153442383\n",
      "epoch: 276 loss: 3.8515536785125732\n",
      "epoch: 277 loss: 3.8515024185180664\n",
      "epoch: 278 loss: 3.8514482975006104\n",
      "epoch: 279 loss: 3.8513917922973633\n",
      "epoch: 280 loss: 3.8513343334198\n",
      "epoch: 281 loss: 3.8512744903564453\n",
      "epoch: 282 loss: 3.851210832595825\n",
      "epoch: 283 loss: 3.851147174835205\n",
      "epoch: 284 loss: 3.851078987121582\n",
      "epoch: 285 loss: 3.851008892059326\n",
      "epoch: 286 loss: 3.8509349822998047\n",
      "epoch: 287 loss: 3.850858211517334\n",
      "epoch: 288 loss: 3.8507776260375977\n",
      "epoch: 289 loss: 3.8506946563720703\n",
      "epoch: 290 loss: 3.850606918334961\n",
      "epoch: 291 loss: 3.8505160808563232\n",
      "epoch: 292 loss: 3.85042142868042\n",
      "epoch: 293 loss: 3.85032320022583\n",
      "epoch: 294 loss: 3.8502204418182373\n",
      "epoch: 295 loss: 3.8501136302948\n",
      "epoch: 296 loss: 3.8500025272369385\n",
      "epoch: 297 loss: 3.8498876094818115\n",
      "epoch: 298 loss: 3.8497681617736816\n",
      "epoch: 299 loss: 3.8496439456939697\n",
      "epoch: 300 loss: 3.8495137691497803\n",
      "epoch: 301 loss: 3.8493804931640625\n",
      "epoch: 302 loss: 3.8492419719696045\n",
      "epoch: 303 loss: 3.8490982055664062\n",
      "epoch: 304 loss: 3.8489480018615723\n",
      "epoch: 305 loss: 3.848795175552368\n",
      "epoch: 306 loss: 3.848635673522949\n",
      "epoch: 307 loss: 3.8484692573547363\n",
      "epoch: 308 loss: 3.848299503326416\n",
      "epoch: 309 loss: 3.8481223583221436\n",
      "epoch: 310 loss: 3.8479394912719727\n",
      "epoch: 311 loss: 3.8477513790130615\n",
      "epoch: 312 loss: 3.8475568294525146\n",
      "epoch: 313 loss: 3.847357749938965\n",
      "epoch: 314 loss: 3.8471527099609375\n",
      "epoch: 315 loss: 3.8469414710998535\n",
      "epoch: 316 loss: 3.846726179122925\n",
      "epoch: 317 loss: 3.8465054035186768\n",
      "epoch: 318 loss: 3.846280097961426\n",
      "epoch: 319 loss: 3.846050500869751\n",
      "epoch: 320 loss: 3.8458163738250732\n",
      "epoch: 321 loss: 3.845578670501709\n",
      "epoch: 322 loss: 3.845339298248291\n",
      "epoch: 323 loss: 3.845094919204712\n",
      "epoch: 324 loss: 3.8448476791381836\n",
      "epoch: 325 loss: 3.8445961475372314\n",
      "epoch: 326 loss: 3.8443427085876465\n",
      "epoch: 327 loss: 3.8440845012664795\n",
      "epoch: 328 loss: 3.8438222408294678\n",
      "epoch: 329 loss: 3.843555212020874\n",
      "epoch: 330 loss: 3.8432810306549072\n",
      "epoch: 331 loss: 3.8430001735687256\n",
      "epoch: 332 loss: 3.842710494995117\n",
      "epoch: 333 loss: 3.842411518096924\n",
      "epoch: 334 loss: 3.8421006202697754\n",
      "epoch: 335 loss: 3.841780185699463\n",
      "epoch: 336 loss: 3.8414506912231445\n",
      "epoch: 337 loss: 3.841113328933716\n",
      "epoch: 338 loss: 3.840771436691284\n",
      "epoch: 339 loss: 3.8404288291931152\n",
      "epoch: 340 loss: 3.8400845527648926\n",
      "epoch: 341 loss: 3.8397443294525146\n",
      "epoch: 342 loss: 3.8394076824188232\n",
      "epoch: 343 loss: 3.8390743732452393\n",
      "epoch: 344 loss: 3.8387451171875\n",
      "epoch: 345 loss: 3.8384203910827637\n",
      "epoch: 346 loss: 3.8381011486053467\n",
      "epoch: 347 loss: 3.8377878665924072\n",
      "epoch: 348 loss: 3.8374783992767334\n",
      "epoch: 349 loss: 3.8371732234954834\n",
      "epoch: 350 loss: 3.836874008178711\n",
      "epoch: 351 loss: 3.8365790843963623\n",
      "epoch: 352 loss: 3.836289405822754\n",
      "epoch: 353 loss: 3.8360021114349365\n",
      "epoch: 354 loss: 3.8357160091400146\n",
      "epoch: 355 loss: 3.835432529449463\n",
      "epoch: 356 loss: 3.835148811340332\n",
      "epoch: 357 loss: 3.8348681926727295\n",
      "epoch: 358 loss: 3.8345913887023926\n",
      "epoch: 359 loss: 3.834317922592163\n",
      "epoch: 360 loss: 3.8340485095977783\n",
      "epoch: 361 loss: 3.833782434463501\n",
      "epoch: 362 loss: 3.8335211277008057\n",
      "epoch: 363 loss: 3.833263874053955\n",
      "epoch: 364 loss: 3.833012104034424\n",
      "epoch: 365 loss: 3.832761764526367\n",
      "epoch: 366 loss: 3.8325154781341553\n",
      "epoch: 367 loss: 3.832275152206421\n",
      "epoch: 368 loss: 3.8320364952087402\n",
      "epoch: 369 loss: 3.8318023681640625\n",
      "epoch: 370 loss: 3.831570625305176\n",
      "epoch: 371 loss: 3.83134126663208\n",
      "epoch: 372 loss: 3.83111572265625\n",
      "epoch: 373 loss: 3.8308911323547363\n",
      "epoch: 374 loss: 3.8306689262390137\n",
      "epoch: 375 loss: 3.8304483890533447\n",
      "epoch: 376 loss: 3.8302292823791504\n",
      "epoch: 377 loss: 3.830012559890747\n",
      "epoch: 378 loss: 3.829796075820923\n",
      "epoch: 379 loss: 3.8295812606811523\n",
      "epoch: 380 loss: 3.829367160797119\n",
      "epoch: 381 loss: 3.8291540145874023\n",
      "epoch: 382 loss: 3.8289411067962646\n",
      "epoch: 383 loss: 3.8287274837493896\n",
      "epoch: 384 loss: 3.8285160064697266\n",
      "epoch: 385 loss: 3.828303813934326\n",
      "epoch: 386 loss: 3.828092575073242\n",
      "epoch: 387 loss: 3.8278818130493164\n",
      "epoch: 388 loss: 3.8276705741882324\n",
      "epoch: 389 loss: 3.8274600505828857\n",
      "epoch: 390 loss: 3.827249050140381\n",
      "epoch: 391 loss: 3.8270370960235596\n",
      "epoch: 392 loss: 3.8268258571624756\n",
      "epoch: 393 loss: 3.826613664627075\n",
      "epoch: 394 loss: 3.826401948928833\n",
      "epoch: 395 loss: 3.826190710067749\n",
      "epoch: 396 loss: 3.8259778022766113\n",
      "epoch: 397 loss: 3.8257663249969482\n",
      "epoch: 398 loss: 3.825552463531494\n",
      "epoch: 399 loss: 3.8253395557403564\n",
      "epoch: 400 loss: 3.8251256942749023\n",
      "epoch: 401 loss: 3.824911594390869\n",
      "epoch: 402 loss: 3.8246963024139404\n",
      "epoch: 403 loss: 3.8244800567626953\n",
      "epoch: 404 loss: 3.824265241622925\n",
      "epoch: 405 loss: 3.824047565460205\n",
      "epoch: 406 loss: 3.823831796646118\n",
      "epoch: 407 loss: 3.8236145973205566\n",
      "epoch: 408 loss: 3.8233957290649414\n",
      "epoch: 409 loss: 3.823176622390747\n",
      "epoch: 410 loss: 3.8229568004608154\n",
      "epoch: 411 loss: 3.822737216949463\n",
      "epoch: 412 loss: 3.822517156600952\n",
      "epoch: 413 loss: 3.8222954273223877\n",
      "epoch: 414 loss: 3.8220746517181396\n",
      "epoch: 415 loss: 3.821852684020996\n",
      "epoch: 416 loss: 3.821629285812378\n",
      "epoch: 417 loss: 3.821406841278076\n",
      "epoch: 418 loss: 3.82118558883667\n",
      "epoch: 419 loss: 3.820963144302368\n",
      "epoch: 420 loss: 3.8207404613494873\n",
      "epoch: 421 loss: 3.8205177783966064\n",
      "epoch: 422 loss: 3.820297956466675\n",
      "epoch: 423 loss: 3.8200759887695312\n",
      "epoch: 424 loss: 3.8198556900024414\n",
      "epoch: 425 loss: 3.8196356296539307\n",
      "epoch: 426 loss: 3.819418430328369\n",
      "epoch: 427 loss: 3.8191986083984375\n",
      "epoch: 428 loss: 3.8189821243286133\n",
      "epoch: 429 loss: 3.8187665939331055\n",
      "epoch: 430 loss: 3.818551540374756\n",
      "epoch: 431 loss: 3.818338632583618\n",
      "epoch: 432 loss: 3.8181264400482178\n",
      "epoch: 433 loss: 3.817915916442871\n",
      "epoch: 434 loss: 3.8177075386047363\n",
      "epoch: 435 loss: 3.8174991607666016\n",
      "epoch: 436 loss: 3.8172922134399414\n",
      "epoch: 437 loss: 3.8170888423919678\n",
      "epoch: 438 loss: 3.8168866634368896\n",
      "epoch: 439 loss: 3.8166842460632324\n",
      "epoch: 440 loss: 3.8164865970611572\n",
      "epoch: 441 loss: 3.816288471221924\n",
      "epoch: 442 loss: 3.816093683242798\n",
      "epoch: 443 loss: 3.8159008026123047\n",
      "epoch: 444 loss: 3.8157079219818115\n",
      "epoch: 445 loss: 3.8155205249786377\n",
      "epoch: 446 loss: 3.815333366394043\n",
      "epoch: 447 loss: 3.815147638320923\n",
      "epoch: 448 loss: 3.8149635791778564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 449 loss: 3.814781904220581\n",
      "epoch: 450 loss: 3.8146026134490967\n",
      "epoch: 451 loss: 3.814424514770508\n",
      "epoch: 452 loss: 3.814248561859131\n",
      "epoch: 453 loss: 3.8140740394592285\n",
      "epoch: 454 loss: 3.813903570175171\n",
      "epoch: 455 loss: 3.8137335777282715\n",
      "epoch: 456 loss: 3.813565731048584\n",
      "epoch: 457 loss: 3.8134000301361084\n",
      "epoch: 458 loss: 3.813235282897949\n",
      "epoch: 459 loss: 3.8130743503570557\n",
      "epoch: 460 loss: 3.8129141330718994\n",
      "epoch: 461 loss: 3.812755823135376\n",
      "epoch: 462 loss: 3.8125998973846436\n",
      "epoch: 463 loss: 3.8124451637268066\n",
      "epoch: 464 loss: 3.8122928142547607\n",
      "epoch: 465 loss: 3.812142848968506\n",
      "epoch: 466 loss: 3.8119940757751465\n",
      "epoch: 467 loss: 3.81184720993042\n",
      "epoch: 468 loss: 3.8117024898529053\n",
      "epoch: 469 loss: 3.8115592002868652\n",
      "epoch: 470 loss: 3.8114185333251953\n",
      "epoch: 471 loss: 3.811277389526367\n",
      "epoch: 472 loss: 3.811140775680542\n",
      "epoch: 473 loss: 3.8110039234161377\n",
      "epoch: 474 loss: 3.8108696937561035\n",
      "epoch: 475 loss: 3.8107383251190186\n",
      "epoch: 476 loss: 3.810605525970459\n",
      "epoch: 477 loss: 3.810473918914795\n",
      "epoch: 478 loss: 3.8103456497192383\n",
      "epoch: 479 loss: 3.81021785736084\n",
      "epoch: 480 loss: 3.8100903034210205\n",
      "epoch: 481 loss: 3.8099656105041504\n",
      "epoch: 482 loss: 3.8098409175872803\n",
      "epoch: 483 loss: 3.8097167015075684\n",
      "epoch: 484 loss: 3.8095943927764893\n",
      "epoch: 485 loss: 3.8094725608825684\n",
      "epoch: 486 loss: 3.809354543685913\n",
      "epoch: 487 loss: 3.809234619140625\n",
      "epoch: 488 loss: 3.8091177940368652\n",
      "epoch: 489 loss: 3.8090004920959473\n",
      "epoch: 490 loss: 3.8088855743408203\n",
      "epoch: 491 loss: 3.8087716102600098\n",
      "epoch: 492 loss: 3.808656930923462\n",
      "epoch: 493 loss: 3.8085455894470215\n",
      "epoch: 494 loss: 3.8084349632263184\n",
      "epoch: 495 loss: 3.8083255290985107\n",
      "epoch: 496 loss: 3.8082165718078613\n",
      "epoch: 497 loss: 3.8081090450286865\n",
      "epoch: 498 loss: 3.8080031871795654\n",
      "epoch: 499 loss: 3.8078982830047607\n",
      "epoch: 500 loss: 3.807793617248535\n",
      "epoch: 501 loss: 3.807687997817993\n",
      "epoch: 502 loss: 3.8075876235961914\n",
      "epoch: 503 loss: 3.807490348815918\n",
      "epoch: 504 loss: 3.807389736175537\n",
      "epoch: 505 loss: 3.8072872161865234\n",
      "epoch: 506 loss: 3.8071913719177246\n",
      "epoch: 507 loss: 3.807098150253296\n",
      "epoch: 508 loss: 3.807002067565918\n",
      "epoch: 509 loss: 3.8069067001342773\n",
      "epoch: 510 loss: 3.806811571121216\n",
      "epoch: 511 loss: 3.806720495223999\n",
      "epoch: 512 loss: 3.8066277503967285\n",
      "epoch: 513 loss: 3.806535005569458\n",
      "epoch: 514 loss: 3.806443452835083\n",
      "epoch: 515 loss: 3.8063554763793945\n",
      "epoch: 516 loss: 3.8062667846679688\n",
      "epoch: 517 loss: 3.806178331375122\n",
      "epoch: 518 loss: 3.8060879707336426\n",
      "epoch: 519 loss: 3.806004047393799\n",
      "epoch: 520 loss: 3.8059191703796387\n",
      "epoch: 521 loss: 3.805832624435425\n",
      "epoch: 522 loss: 3.8057498931884766\n",
      "epoch: 523 loss: 3.8056702613830566\n",
      "epoch: 524 loss: 3.805590867996216\n",
      "epoch: 525 loss: 3.805509328842163\n",
      "epoch: 526 loss: 3.8054285049438477\n",
      "epoch: 527 loss: 3.8053534030914307\n",
      "epoch: 528 loss: 3.8052783012390137\n",
      "epoch: 529 loss: 3.805201530456543\n",
      "epoch: 530 loss: 3.8051223754882812\n",
      "epoch: 531 loss: 3.805051803588867\n",
      "epoch: 532 loss: 3.8049817085266113\n",
      "epoch: 533 loss: 3.8049089908599854\n",
      "epoch: 534 loss: 3.804835796356201\n",
      "epoch: 535 loss: 3.8047666549682617\n",
      "epoch: 536 loss: 3.8047029972076416\n",
      "epoch: 537 loss: 3.804631233215332\n",
      "epoch: 538 loss: 3.8045601844787598\n",
      "epoch: 539 loss: 3.804494857788086\n",
      "epoch: 540 loss: 3.8044357299804688\n",
      "epoch: 541 loss: 3.804368019104004\n",
      "epoch: 542 loss: 3.804302453994751\n",
      "epoch: 543 loss: 3.8042385578155518\n",
      "epoch: 544 loss: 3.8041815757751465\n",
      "epoch: 545 loss: 3.8041200637817383\n",
      "epoch: 546 loss: 3.8040544986724854\n",
      "epoch: 547 loss: 3.8039934635162354\n",
      "epoch: 548 loss: 3.8039422035217285\n",
      "epoch: 549 loss: 3.8038830757141113\n",
      "epoch: 550 loss: 3.803820848464966\n",
      "epoch: 551 loss: 3.803760290145874\n",
      "epoch: 552 loss: 3.8037116527557373\n",
      "epoch: 553 loss: 3.803656578063965\n",
      "epoch: 554 loss: 3.8035964965820312\n",
      "epoch: 555 loss: 3.803539514541626\n",
      "epoch: 556 loss: 3.8034939765930176\n",
      "epoch: 557 loss: 3.8034403324127197\n",
      "epoch: 558 loss: 3.80338191986084\n",
      "epoch: 559 loss: 3.803328275680542\n",
      "epoch: 560 loss: 3.803283929824829\n",
      "epoch: 561 loss: 3.8032338619232178\n",
      "epoch: 562 loss: 3.803178071975708\n",
      "epoch: 563 loss: 3.803126335144043\n",
      "epoch: 564 loss: 3.8030855655670166\n",
      "epoch: 565 loss: 3.803037405014038\n",
      "epoch: 566 loss: 3.8029842376708984\n",
      "epoch: 567 loss: 3.8029356002807617\n",
      "epoch: 568 loss: 3.8028979301452637\n",
      "epoch: 569 loss: 3.8028502464294434\n",
      "epoch: 570 loss: 3.8027989864349365\n",
      "epoch: 571 loss: 3.8027541637420654\n",
      "epoch: 572 loss: 3.802720308303833\n",
      "epoch: 573 loss: 3.8026721477508545\n",
      "epoch: 574 loss: 3.802624225616455\n",
      "epoch: 575 loss: 3.802584171295166\n",
      "epoch: 576 loss: 3.8025519847869873\n",
      "epoch: 577 loss: 3.802506685256958\n",
      "epoch: 578 loss: 3.802459478378296\n",
      "epoch: 579 loss: 3.8024260997772217\n",
      "epoch: 580 loss: 3.802395820617676\n",
      "epoch: 581 loss: 3.8023505210876465\n",
      "epoch: 582 loss: 3.802309989929199\n",
      "epoch: 583 loss: 3.802280902862549\n",
      "epoch: 584 loss: 3.8022525310516357\n",
      "epoch: 585 loss: 3.80220890045166\n",
      "epoch: 586 loss: 3.802172899246216\n",
      "epoch: 587 loss: 3.802152395248413\n",
      "epoch: 588 loss: 3.8021206855773926\n",
      "epoch: 589 loss: 3.802083730697632\n",
      "epoch: 590 loss: 3.8020520210266113\n",
      "epoch: 591 loss: 3.802037000656128\n",
      "epoch: 592 loss: 3.802004337310791\n",
      "epoch: 593 loss: 3.801971197128296\n",
      "epoch: 594 loss: 3.8019471168518066\n",
      "epoch: 595 loss: 3.8019375801086426\n",
      "epoch: 596 loss: 3.8019020557403564\n",
      "epoch: 597 loss: 3.8018767833709717\n",
      "epoch: 598 loss: 3.801858425140381\n",
      "epoch: 599 loss: 3.8018484115600586\n",
      "epoch: 600 loss: 3.801814079284668\n",
      "epoch: 601 loss: 3.8017959594726562\n",
      "epoch: 602 loss: 3.8017868995666504\n",
      "epoch: 603 loss: 3.801772356033325\n",
      "epoch: 604 loss: 3.801743268966675\n",
      "epoch: 605 loss: 3.8017325401306152\n",
      "epoch: 606 loss: 3.8017282485961914\n",
      "epoch: 607 loss: 3.8017101287841797\n",
      "epoch: 608 loss: 3.801689386367798\n",
      "epoch: 609 loss: 3.801687002182007\n",
      "epoch: 610 loss: 3.8016836643218994\n",
      "epoch: 611 loss: 3.8016610145568848\n",
      "epoch: 612 loss: 3.8016486167907715\n",
      "epoch: 613 loss: 3.801654100418091\n",
      "epoch: 614 loss: 3.8016440868377686\n",
      "epoch: 615 loss: 3.8016278743743896\n",
      "epoch: 616 loss: 3.8016223907470703\n",
      "epoch: 617 loss: 3.8016295433044434\n",
      "epoch: 618 loss: 3.801614284515381\n",
      "epoch: 619 loss: 3.801605224609375\n",
      "epoch: 620 loss: 3.801607131958008\n",
      "epoch: 621 loss: 3.8016138076782227\n",
      "epoch: 622 loss: 3.8015947341918945\n",
      "epoch: 623 loss: 3.8015944957733154\n",
      "epoch: 624 loss: 3.801602840423584\n",
      "epoch: 625 loss: 3.8015987873077393\n",
      "epoch: 626 loss: 3.801589012145996\n",
      "epoch: 627 loss: 3.801595449447632\n",
      "epoch: 628 loss: 3.8016107082366943\n",
      "epoch: 629 loss: 3.8016021251678467\n",
      "epoch: 630 loss: 3.801600694656372\n",
      "epoch: 631 loss: 3.8016197681427\n",
      "epoch: 632 loss: 3.801626682281494\n",
      "epoch: 633 loss: 3.8016245365142822\n",
      "epoch: 634 loss: 3.8016364574432373\n",
      "epoch: 635 loss: 3.801659345626831\n",
      "epoch: 636 loss: 3.801656723022461\n",
      "epoch: 637 loss: 3.801666498184204\n",
      "epoch: 638 loss: 3.8016862869262695\n",
      "epoch: 639 loss: 3.801706552505493\n",
      "epoch: 640 loss: 3.801703453063965\n",
      "epoch: 641 loss: 3.8017215728759766\n",
      "epoch: 642 loss: 3.801748752593994\n",
      "epoch: 643 loss: 3.801757574081421\n",
      "epoch: 644 loss: 3.8017656803131104\n",
      "epoch: 645 loss: 3.801792621612549\n",
      "epoch: 646 loss: 3.801816940307617\n",
      "epoch: 647 loss: 3.8018198013305664\n",
      "epoch: 648 loss: 3.8018383979797363\n",
      "epoch: 649 loss: 3.801870346069336\n",
      "epoch: 650 loss: 3.801882028579712\n",
      "epoch: 651 loss: 3.8018929958343506\n",
      "epoch: 652 loss: 3.8019208908081055\n",
      "epoch: 653 loss: 3.8019495010375977\n",
      "epoch: 654 loss: 3.8019533157348633\n",
      "epoch: 655 loss: 3.8019726276397705\n",
      "epoch: 656 loss: 3.8020074367523193\n",
      "epoch: 657 loss: 3.802020788192749\n",
      "epoch: 658 loss: 3.8020293712615967\n",
      "epoch: 659 loss: 3.8020589351654053\n",
      "epoch: 660 loss: 3.8020870685577393\n",
      "epoch: 661 loss: 3.802091121673584\n",
      "epoch: 662 loss: 3.802109479904175\n",
      "epoch: 663 loss: 3.8021433353424072\n",
      "epoch: 664 loss: 3.802154302597046\n",
      "epoch: 665 loss: 3.802163600921631\n",
      "epoch: 666 loss: 3.802191972732544\n",
      "epoch: 667 loss: 3.802215814590454\n",
      "epoch: 668 loss: 3.8022167682647705\n",
      "epoch: 669 loss: 3.8022372722625732\n",
      "epoch: 670 loss: 3.8022682666778564\n",
      "epoch: 671 loss: 3.802274703979492\n",
      "epoch: 672 loss: 3.8022820949554443\n",
      "epoch: 673 loss: 3.8023080825805664\n",
      "epoch: 674 loss: 3.8023288249969482\n",
      "epoch: 675 loss: 3.8023242950439453\n",
      "epoch: 676 loss: 3.8023428916931152\n",
      "epoch: 677 loss: 3.802370071411133\n",
      "epoch: 678 loss: 3.802367925643921\n",
      "epoch: 679 loss: 3.8023722171783447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 680 loss: 3.8024001121520996\n",
      "epoch: 681 loss: 3.8024075031280518\n",
      "epoch: 682 loss: 3.8024020195007324\n",
      "epoch: 683 loss: 3.802417278289795\n",
      "epoch: 684 loss: 3.802435874938965\n",
      "epoch: 685 loss: 3.8024258613586426\n",
      "epoch: 686 loss: 3.8024299144744873\n",
      "epoch: 687 loss: 3.8024489879608154\n",
      "epoch: 688 loss: 3.8024446964263916\n",
      "epoch: 689 loss: 3.802433967590332\n",
      "epoch: 690 loss: 3.8024489879608154\n",
      "epoch: 691 loss: 3.8024518489837646\n",
      "epoch: 692 loss: 3.8024353981018066\n",
      "epoch: 693 loss: 3.802433490753174\n",
      "epoch: 694 loss: 3.80244517326355\n",
      "epoch: 695 loss: 3.802424907684326\n",
      "epoch: 696 loss: 3.802412509918213\n",
      "epoch: 697 loss: 3.802415370941162\n",
      "epoch: 698 loss: 3.802408218383789\n",
      "epoch: 699 loss: 3.8023765087127686\n",
      "epoch: 700 loss: 3.802374839782715\n",
      "epoch: 701 loss: 3.802367925643921\n",
      "epoch: 702 loss: 3.802335739135742\n",
      "epoch: 703 loss: 3.802314519882202\n",
      "epoch: 704 loss: 3.8023135662078857\n",
      "epoch: 705 loss: 3.802276372909546\n",
      "epoch: 706 loss: 3.80224609375\n",
      "epoch: 707 loss: 3.802227735519409\n",
      "epoch: 708 loss: 3.8022098541259766\n",
      "epoch: 709 loss: 3.8021557331085205\n",
      "epoch: 710 loss: 3.802138566970825\n",
      "epoch: 711 loss: 3.802107334136963\n",
      "epoch: 712 loss: 3.8020620346069336\n",
      "epoch: 713 loss: 3.8020131587982178\n",
      "epoch: 714 loss: 3.8020007610321045\n",
      "epoch: 715 loss: 3.8019397258758545\n",
      "epoch: 716 loss: 3.8018922805786133\n",
      "epoch: 717 loss: 3.8018507957458496\n",
      "epoch: 718 loss: 3.801816463470459\n",
      "epoch: 719 loss: 3.801736354827881\n",
      "epoch: 720 loss: 3.801699638366699\n",
      "epoch: 721 loss: 3.801647901535034\n",
      "epoch: 722 loss: 3.801582098007202\n",
      "epoch: 723 loss: 3.801508903503418\n",
      "epoch: 724 loss: 3.8014767169952393\n",
      "epoch: 725 loss: 3.8013904094696045\n",
      "epoch: 726 loss: 3.801323890686035\n",
      "epoch: 727 loss: 3.8012585639953613\n",
      "epoch: 728 loss: 3.801204204559326\n",
      "epoch: 729 loss: 3.801096200942993\n",
      "epoch: 730 loss: 3.801041603088379\n",
      "epoch: 731 loss: 3.8009636402130127\n",
      "epoch: 732 loss: 3.8008785247802734\n",
      "epoch: 733 loss: 3.8007779121398926\n",
      "epoch: 734 loss: 3.8007287979125977\n",
      "epoch: 735 loss: 3.8006136417388916\n",
      "epoch: 736 loss: 3.800524950027466\n",
      "epoch: 737 loss: 3.8004331588745117\n",
      "epoch: 738 loss: 3.8003571033477783\n",
      "epoch: 739 loss: 3.800222158432007\n",
      "epoch: 740 loss: 3.80014705657959\n",
      "epoch: 741 loss: 3.8000404834747314\n",
      "epoch: 742 loss: 3.7999281883239746\n",
      "epoch: 743 loss: 3.7998034954071045\n",
      "epoch: 744 loss: 3.7997305393218994\n",
      "epoch: 745 loss: 3.7995848655700684\n",
      "epoch: 746 loss: 3.7994773387908936\n",
      "epoch: 747 loss: 3.7993578910827637\n",
      "epoch: 748 loss: 3.7992565631866455\n",
      "epoch: 749 loss: 3.799095392227173\n",
      "epoch: 750 loss: 3.7990050315856934\n",
      "epoch: 751 loss: 3.7988717555999756\n",
      "epoch: 752 loss: 3.7987422943115234\n",
      "epoch: 753 loss: 3.7985994815826416\n",
      "epoch: 754 loss: 3.7985074520111084\n",
      "epoch: 755 loss: 3.7983343601226807\n",
      "epoch: 756 loss: 3.798220157623291\n",
      "epoch: 757 loss: 3.7980802059173584\n",
      "epoch: 758 loss: 3.797961473464966\n",
      "epoch: 759 loss: 3.7977843284606934\n",
      "epoch: 760 loss: 3.7976880073547363\n",
      "epoch: 761 loss: 3.79752779006958\n",
      "epoch: 762 loss: 3.7973885536193848\n",
      "epoch: 763 loss: 3.7972335815429688\n",
      "epoch: 764 loss: 3.7971255779266357\n",
      "epoch: 765 loss: 3.79693603515625\n",
      "epoch: 766 loss: 3.796816825866699\n",
      "epoch: 767 loss: 3.796659469604492\n",
      "epoch: 768 loss: 3.7965281009674072\n",
      "epoch: 769 loss: 3.7963366508483887\n",
      "epoch: 770 loss: 3.7962348461151123\n",
      "epoch: 771 loss: 3.796052932739258\n",
      "epoch: 772 loss: 3.795908212661743\n",
      "epoch: 773 loss: 3.7957448959350586\n",
      "epoch: 774 loss: 3.7956223487854004\n",
      "epoch: 775 loss: 3.795419692993164\n",
      "epoch: 776 loss: 3.795297145843506\n",
      "epoch: 777 loss: 3.7951271533966064\n",
      "epoch: 778 loss: 3.794978618621826\n",
      "epoch: 779 loss: 3.7947890758514404\n",
      "epoch: 780 loss: 3.794680595397949\n",
      "epoch: 781 loss: 3.7944774627685547\n",
      "epoch: 782 loss: 3.7943320274353027\n",
      "epoch: 783 loss: 3.794159173965454\n",
      "epoch: 784 loss: 3.7940280437469482\n",
      "epoch: 785 loss: 3.793814182281494\n",
      "epoch: 786 loss: 3.793696403503418\n",
      "epoch: 787 loss: 3.7935097217559814\n",
      "epoch: 788 loss: 3.7933566570281982\n",
      "epoch: 789 loss: 3.793165922164917\n",
      "epoch: 790 loss: 3.793050765991211\n",
      "epoch: 791 loss: 3.7928338050842285\n",
      "epoch: 792 loss: 3.792698860168457\n",
      "epoch: 793 loss: 3.792515277862549\n",
      "epoch: 794 loss: 3.7923779487609863\n",
      "epoch: 795 loss: 3.7921628952026367\n",
      "epoch: 796 loss: 3.792048931121826\n",
      "epoch: 797 loss: 3.791846990585327\n",
      "epoch: 798 loss: 3.7916977405548096\n",
      "epoch: 799 loss: 3.791508197784424\n",
      "epoch: 800 loss: 3.7913880348205566\n",
      "epoch: 801 loss: 3.7911627292633057\n",
      "epoch: 802 loss: 3.7910385131835938\n",
      "epoch: 803 loss: 3.7908449172973633\n",
      "epoch: 804 loss: 3.7907073497772217\n",
      "epoch: 805 loss: 3.7904882431030273\n",
      "epoch: 806 loss: 3.7903847694396973\n",
      "epoch: 807 loss: 3.7901687622070312\n",
      "epoch: 808 loss: 3.7900261878967285\n",
      "epoch: 809 loss: 3.7898406982421875\n",
      "epoch: 810 loss: 3.7897140979766846\n",
      "epoch: 811 loss: 3.7894887924194336\n",
      "epoch: 812 loss: 3.7893738746643066\n",
      "epoch: 813 loss: 3.7891759872436523\n",
      "epoch: 814 loss: 3.7890372276306152\n",
      "epoch: 815 loss: 3.7888259887695312\n",
      "epoch: 816 loss: 3.7887279987335205\n",
      "epoch: 817 loss: 3.788503646850586\n",
      "epoch: 818 loss: 3.788372755050659\n",
      "epoch: 819 loss: 3.7881860733032227\n",
      "epoch: 820 loss: 3.788060188293457\n",
      "epoch: 821 loss: 3.7878363132476807\n",
      "epoch: 822 loss: 3.7877347469329834\n",
      "epoch: 823 loss: 3.787529706954956\n",
      "epoch: 824 loss: 3.7873945236206055\n",
      "epoch: 825 loss: 3.7871904373168945\n",
      "epoch: 826 loss: 3.787097692489624\n",
      "epoch: 827 loss: 3.786865234375\n",
      "epoch: 828 loss: 3.7867496013641357\n",
      "epoch: 829 loss: 3.786557197570801\n",
      "epoch: 830 loss: 3.786442518234253\n",
      "epoch: 831 loss: 3.7862164974212646\n",
      "epoch: 832 loss: 3.786123752593994\n",
      "epoch: 833 loss: 3.7859151363372803\n",
      "epoch: 834 loss: 3.7857930660247803\n",
      "epoch: 835 loss: 3.785590410232544\n",
      "epoch: 836 loss: 3.7855007648468018\n",
      "epoch: 837 loss: 3.7852697372436523\n",
      "epoch: 838 loss: 3.785166025161743\n",
      "epoch: 839 loss: 3.784966230392456\n",
      "epoch: 840 loss: 3.7848641872406006\n",
      "epoch: 841 loss: 3.7846388816833496\n",
      "epoch: 842 loss: 3.7845547199249268\n",
      "epoch: 843 loss: 3.7843427658081055\n",
      "epoch: 844 loss: 3.784236192703247\n",
      "epoch: 845 loss: 3.784022808074951\n",
      "epoch: 846 loss: 3.783944606781006\n",
      "epoch: 847 loss: 3.7837162017822266\n",
      "epoch: 848 loss: 3.7836217880249023\n",
      "epoch: 849 loss: 3.783414840698242\n",
      "epoch: 850 loss: 3.7833287715911865\n",
      "epoch: 851 loss: 3.7830991744995117\n",
      "epoch: 852 loss: 3.7830214500427246\n",
      "epoch: 853 loss: 3.7828056812286377\n",
      "epoch: 854 loss: 3.782719612121582\n",
      "epoch: 855 loss: 3.7824947834014893\n",
      "epoch: 856 loss: 3.7824225425720215\n",
      "epoch: 857 loss: 3.782198190689087\n",
      "epoch: 858 loss: 3.7821154594421387\n",
      "epoch: 859 loss: 3.7818965911865234\n",
      "epoch: 860 loss: 3.78182315826416\n",
      "epoch: 861 loss: 3.7815966606140137\n",
      "epoch: 862 loss: 3.781520128250122\n",
      "epoch: 863 loss: 3.7813005447387695\n",
      "epoch: 864 loss: 3.781226873397827\n",
      "epoch: 865 loss: 3.7809996604919434\n",
      "epoch: 866 loss: 3.7809267044067383\n",
      "epoch: 867 loss: 3.780705451965332\n",
      "epoch: 868 loss: 3.7806334495544434\n",
      "epoch: 869 loss: 3.780405282974243\n",
      "epoch: 870 loss: 3.780337333679199\n",
      "epoch: 871 loss: 3.7801144123077393\n",
      "epoch: 872 loss: 3.7800381183624268\n",
      "epoch: 873 loss: 3.7798187732696533\n",
      "epoch: 874 loss: 3.7797441482543945\n",
      "epoch: 875 loss: 3.779522180557251\n",
      "epoch: 876 loss: 3.7794501781463623\n",
      "epoch: 877 loss: 3.7792253494262695\n",
      "epoch: 878 loss: 3.7791543006896973\n",
      "epoch: 879 loss: 3.7789313793182373\n",
      "epoch: 880 loss: 3.7788586616516113\n",
      "epoch: 881 loss: 3.7786355018615723\n",
      "epoch: 882 loss: 3.7785651683807373\n",
      "epoch: 883 loss: 3.7783396244049072\n",
      "epoch: 884 loss: 3.778266668319702\n",
      "epoch: 885 loss: 3.7780416011810303\n",
      "epoch: 886 loss: 3.777970790863037\n",
      "epoch: 887 loss: 3.777743339538574\n",
      "epoch: 888 loss: 3.7776710987091064\n",
      "epoch: 889 loss: 3.7774431705474854\n",
      "epoch: 890 loss: 3.777371883392334\n",
      "epoch: 891 loss: 3.7771432399749756\n",
      "epoch: 892 loss: 3.777069091796875\n",
      "epoch: 893 loss: 3.776841640472412\n",
      "epoch: 894 loss: 3.7767672538757324\n",
      "epoch: 895 loss: 3.7765398025512695\n",
      "epoch: 896 loss: 3.776463508605957\n",
      "epoch: 897 loss: 3.776233434677124\n",
      "epoch: 898 loss: 3.776156425476074\n",
      "epoch: 899 loss: 3.77592396736145\n",
      "epoch: 900 loss: 3.7758471965789795\n",
      "epoch: 901 loss: 3.7756171226501465\n",
      "epoch: 902 loss: 3.775535821914673\n",
      "epoch: 903 loss: 3.7753052711486816\n",
      "epoch: 904 loss: 3.775223731994629\n",
      "epoch: 905 loss: 3.7749884128570557\n",
      "epoch: 906 loss: 3.774907112121582\n",
      "epoch: 907 loss: 3.7746710777282715\n",
      "epoch: 908 loss: 3.7745838165283203\n",
      "epoch: 909 loss: 3.774348497390747\n",
      "epoch: 910 loss: 3.7742621898651123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 911 loss: 3.7740235328674316\n",
      "epoch: 912 loss: 3.7739369869232178\n",
      "epoch: 913 loss: 3.7736949920654297\n",
      "epoch: 914 loss: 3.773606061935425\n",
      "epoch: 915 loss: 3.773364305496216\n",
      "epoch: 916 loss: 3.773268938064575\n",
      "epoch: 917 loss: 3.7730283737182617\n",
      "epoch: 918 loss: 3.7729341983795166\n",
      "epoch: 919 loss: 3.772690534591675\n",
      "epoch: 920 loss: 3.7725894451141357\n",
      "epoch: 921 loss: 3.7723464965820312\n",
      "epoch: 922 loss: 3.7722485065460205\n",
      "epoch: 923 loss: 3.7719995975494385\n",
      "epoch: 924 loss: 3.7719011306762695\n",
      "epoch: 925 loss: 3.7716524600982666\n",
      "epoch: 926 loss: 3.7715468406677246\n",
      "epoch: 927 loss: 3.771299123764038\n",
      "epoch: 928 loss: 3.771193504333496\n",
      "epoch: 929 loss: 3.770941734313965\n",
      "epoch: 930 loss: 3.7708401679992676\n",
      "epoch: 931 loss: 3.770582914352417\n",
      "epoch: 932 loss: 3.7704784870147705\n",
      "epoch: 933 loss: 3.770223617553711\n",
      "epoch: 934 loss: 3.770108699798584\n",
      "epoch: 935 loss: 3.7698581218719482\n",
      "epoch: 936 loss: 3.769742727279663\n",
      "epoch: 937 loss: 3.769488573074341\n",
      "epoch: 938 loss: 3.7693772315979004\n",
      "epoch: 939 loss: 3.7691171169281006\n",
      "epoch: 940 loss: 3.76900053024292\n",
      "epoch: 941 loss: 3.7687439918518066\n",
      "epoch: 942 loss: 3.7686245441436768\n",
      "epoch: 943 loss: 3.7683675289154053\n",
      "epoch: 944 loss: 3.76824951171875\n",
      "epoch: 945 loss: 3.7679903507232666\n",
      "epoch: 946 loss: 3.7678680419921875\n",
      "epoch: 947 loss: 3.7676074504852295\n",
      "epoch: 948 loss: 3.7674856185913086\n",
      "epoch: 949 loss: 3.767220973968506\n",
      "epoch: 950 loss: 3.767096519470215\n",
      "epoch: 951 loss: 3.7668378353118896\n",
      "epoch: 952 loss: 3.7667124271392822\n",
      "epoch: 953 loss: 3.766451835632324\n",
      "epoch: 954 loss: 3.766319990158081\n",
      "epoch: 955 loss: 3.766064167022705\n",
      "epoch: 956 loss: 3.765933036804199\n",
      "epoch: 957 loss: 3.7656748294830322\n",
      "epoch: 958 loss: 3.765537977218628\n",
      "epoch: 959 loss: 3.76527738571167\n",
      "epoch: 960 loss: 3.7651422023773193\n",
      "epoch: 961 loss: 3.7648890018463135\n",
      "epoch: 962 loss: 3.7647478580474854\n",
      "epoch: 963 loss: 3.764488458633423\n",
      "epoch: 964 loss: 3.7643532752990723\n",
      "epoch: 965 loss: 3.7640914916992188\n",
      "epoch: 966 loss: 3.7639577388763428\n",
      "epoch: 967 loss: 3.763695478439331\n",
      "epoch: 968 loss: 3.763552665710449\n",
      "epoch: 969 loss: 3.7632956504821777\n",
      "epoch: 970 loss: 3.763153314590454\n",
      "epoch: 971 loss: 3.7628908157348633\n",
      "epoch: 972 loss: 3.7627506256103516\n",
      "epoch: 973 loss: 3.762495994567871\n",
      "epoch: 974 loss: 3.7623496055603027\n",
      "epoch: 975 loss: 3.7620880603790283\n",
      "epoch: 976 loss: 3.7619454860687256\n",
      "epoch: 977 loss: 3.7616870403289795\n",
      "epoch: 978 loss: 3.761544704437256\n",
      "epoch: 979 loss: 3.7612826824188232\n",
      "epoch: 980 loss: 3.7611355781555176\n",
      "epoch: 981 loss: 3.7608819007873535\n",
      "epoch: 982 loss: 3.7607297897338867\n",
      "epoch: 983 loss: 3.760474443435669\n",
      "epoch: 984 loss: 3.7603249549865723\n",
      "epoch: 985 loss: 3.7600677013397217\n",
      "epoch: 986 loss: 3.7599194049835205\n",
      "epoch: 987 loss: 3.7596640586853027\n",
      "epoch: 988 loss: 3.7595105171203613\n",
      "epoch: 989 loss: 3.759261131286621\n",
      "epoch: 990 loss: 3.7591030597686768\n",
      "epoch: 991 loss: 3.758854866027832\n",
      "epoch: 992 loss: 3.7586965560913086\n",
      "epoch: 993 loss: 3.758445978164673\n",
      "epoch: 994 loss: 3.7582895755767822\n",
      "epoch: 995 loss: 3.7580406665802\n",
      "epoch: 996 loss: 3.7578814029693604\n",
      "epoch: 997 loss: 3.757636785507202\n",
      "epoch: 998 loss: 3.757474422454834\n",
      "epoch: 999 loss: 3.7572295665740967\n",
      "epoch: 1000 loss: 3.757065773010254\n",
      "****************Post-Training Accuracy********************\n",
      "______________________________________\n",
      "Non-Fastball rate: 0.42080338266384776\n",
      "Predicted Non-Fastball rate: 0.41923890063424946\n",
      "Non-Fastball accuracy: 0.6752411575562701\n",
      "______________________________________\n",
      "Fastball rate: 0.5791966173361522\n",
      "Predicted Fastball rate: 0.5807610993657505\n",
      "Fastball accuracy: 0.7667542706964521\n",
      "______________________________________\n",
      "Accuracy: 0.7282452431289641\n",
      "Accuracy above naive guess: 0.14904862579281186\n",
      "______________________________________\n",
      "**********************************************************\n"
     ]
    }
   ],
   "source": [
    "model = PitchPredict(PREV_PITCH_DIM, HIDDEN_DIM, NUM_PREV_PITCHES, OUT_DIM, GAME_STATE_DIM, GAME_OUT_DIM, len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    print(\"***************Pre-Training Accuracy*****************\")\n",
    "    test_accuracy(train_batches,model)\n",
    "print(\"**************************Training*****************************\")\n",
    "for epoch in range(1000):\n",
    "    #shuffle(train_batches)\n",
    "    for batch in train_batches:\n",
    "        prev_pitches,pre_pitch, ptypes = batch\n",
    "        \n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        #get input tensors ready\n",
    "        prevs_in = torch.tensor(prev_pitches, dtype=torch.float)\n",
    "        game_state_in = torch.tensor(pre_pitch, dtype=torch.float)\n",
    "        \n",
    "        #get target value\n",
    "        #target = tag_to_ix[ptype]\n",
    "        target = [ tag_to_ix[ptype] for ptype in ptypes]\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model((prevs_in,game_state_in))\n",
    "               \n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, torch.tensor(target,dtype=torch.long))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        preds = [tag_score.max(0) for tag_score in tag_scores]\n",
    "\n",
    "                \n",
    "    #display post-epoch results\n",
    "    print('epoch:',epoch+1,\"loss:\",loss.item())\n",
    "\n",
    "# See what the scores are after training\n",
    "with torch.no_grad():\n",
    "    print(\"****************Post-Training Accuracy********************\")\n",
    "    test_accuracy(train_batches,model.eval())\n",
    "    print(\"**********************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************Validation Accuracy********************\n",
      "______________________________________\n",
      "Non-Fastball rate: 0.41668789808917195\n",
      "Predicted Non-Fastball rate: 0.36738853503184715\n",
      "Non-Fastball accuracy: 0.44451238153469885\n",
      "______________________________________\n",
      "Fastball rate: 0.583312101910828\n",
      "Predicted Fastball rate: 0.6326114649681529\n",
      "Fastball accuracy: 0.6877047390259882\n",
      "______________________________________\n",
      "Accuracy: 0.5863694267515923\n",
      "Accuracy above naive guess: 0.003057324840764264\n",
      "______________________________________\n",
      "**********************************************************\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(\"****************Validation Accuracy********************\")\n",
    "    test_accuracy(validate_batches,model)\n",
    "    print(\"**********************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************Test Accuracy********************\n",
      "______________________________________\n",
      "Non-Fastball rate: 0.4173248407643312\n",
      "Predicted Non-Fastball rate: 0.36560509554140125\n",
      "Non-Fastball accuracy: 0.4548229548229548\n",
      "______________________________________\n",
      "Fastball rate: 0.5826751592356688\n",
      "Predicted Fastball rate: 0.6343949044585987\n",
      "Fastball accuracy: 0.6982947092260603\n",
      "______________________________________\n",
      "Accuracy: 0.596687898089172\n",
      "Accuracy above naive guess: 0.014012738853503182\n",
      "______________________________________\n",
      "**********************************************************\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(\"****************Test Accuracy********************\")\n",
    "    test_accuracy(test_batches,model)\n",
    "    print(\"**********************************************************\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
