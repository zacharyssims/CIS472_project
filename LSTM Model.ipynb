{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspiration for Neural Net: https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb67cbccdf0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pitcher\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zac/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (87,88) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3940: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "player = 'verlander'\n",
    "data = pd.read_csv('Data/raw_data/'+player+'.csv')\n",
    "data = pitcher.clean_data(data)\n",
    "games = pitcher.get_games(data)\n",
    "reps = pitcher.get_reps(games)\n",
    "reps = pitcher.drop_nas(reps)\n",
    "reps = pitcher.drop_pitches(reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run for less fastballs\n",
    "train = pitcher.drop_ff(reps[1:30000])\n",
    "test = reps[30000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run for unbiased distribution\n",
    "train = reps[1:33000]\n",
    "test = reps[33000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work-in-progress\n",
    "def get_batches(train,batch_size):   \n",
    "    out = []\n",
    "    i = 0\n",
    "    batch_ptypes = []\n",
    "    batch_pre_pitch = []\n",
    "    batch_prev_pitches = []\n",
    "    for rep in train:\n",
    "        prev_pitches,prev_types,pre_pitch,ptype = rep\n",
    "        if i % batch_size == 0:\n",
    "            out.append([batch_prev_pitches,batch_pre_pitch,batch_ptypes])\n",
    "            batch_ptypes = [ptype]\n",
    "            batch_pre_pitch = [pre_pitch]\n",
    "            batch_prev_pitches = [prev_pitches]\n",
    "        else:\n",
    "            batch_ptypes.append(ptype)\n",
    "            batch_pre_pitch.append(pre_pitch)\n",
    "            batch_prev_pitches.append(prev_pitches)\n",
    "        i += 1\n",
    "    return out[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = get_batches(train,50)\n",
    "test_batches = get_batches(test,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(batches,model):\n",
    "    length = len(batches)*len(batches[0][0])\n",
    "    num_right = 0\n",
    "    ch_count = 0\n",
    "    predict_ch = 0\n",
    "    cu_count = 0\n",
    "    predict_cu = 0\n",
    "    sl_count = 0\n",
    "    predict_sl = 0\n",
    "    ff_count = 0\n",
    "    predict_ff = 0\n",
    "    for batch in batches:\n",
    "        with torch.no_grad():\n",
    "            prev_pitches ,pre_pitch, ptypes = batch\n",
    "            prevs_in = torch.tensor(prev_pitches, dtype=torch.float)\n",
    "            rep_in = torch.tensor(pre_pitch, dtype=torch.float)\n",
    "            targets = [tag_to_ix[ptype] for ptype in ptypes]\n",
    "            if torch.cuda.is_available():\n",
    "                tag_scores = model((prevs_in,rep_in)).cuda()\n",
    "            else:\n",
    "                tag_scores = model((prevs_in,rep_in))\n",
    "            preds = [tag_score.max(0) for tag_score in tag_scores]\n",
    "            #print(index.item())\n",
    "            for pred,target in zip(preds,targets):\n",
    "                _,index = pred\n",
    "                if target == 1:\n",
    "                    ff_count += 1\n",
    "                if index.item() == 1:\n",
    "                    predict_ff += 1\n",
    "                if target == 0:\n",
    "                    cu_count += 1\n",
    "                if index.item() == 0:\n",
    "                    predict_cu += 1\n",
    "                if target == 2:\n",
    "                    sl_count += 1\n",
    "                if index.item() == 2:\n",
    "                    predict_sl += 1\n",
    "                if target == 3:\n",
    "                    ch_count += 1\n",
    "                if index.item() == 3:\n",
    "                    predict_ch += 1\n",
    "                if index.item() == target:\n",
    "                    num_right += 1   \n",
    "    ff_rate = ff_count/length\n",
    "    pred_ff_rate = predict_ff/length\n",
    "    cu_rate = cu_count/length\n",
    "    pred_cu_rate = predict_cu/length\n",
    "    sl_rate = sl_count/length\n",
    "    pred_sl_rate = predict_sl/length\n",
    "    ch_rate = ch_count/length\n",
    "    pred_ch_rate = predict_ch/length\n",
    "    accuracy = num_right/length\n",
    "    print(\"______________________________________\")\n",
    "    print(\"curve rate:\",cu_rate)\n",
    "    print(\"Predicted curve rate:\",pred_cu_rate)\n",
    "    print(\"______________________________________\")\n",
    "    print(\"Fourseam Fastball rate:\",ff_rate)\n",
    "    print(\"Predicted Fourseam Fastball rate:\",pred_ff_rate)\n",
    "    print(\"______________________________________\")\n",
    "    print(\"Changeup rate:\",ch_rate)\n",
    "    print(\"Predicted changeup rate:\",pred_ch_rate)\n",
    "    print(\"______________________________________\")\n",
    "    print(\"slider rate:\",sl_rate)\n",
    "    print(\"Predicted slider rate:\",pred_sl_rate)\n",
    "    print(\"______________________________________\")\n",
    "    print(\"Accuracy:\",accuracy)\n",
    "    print(\"Accuracy above naive guess:\",accuracy - ff_rate)\n",
    "    print(\"______________________________________\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change this for different pitcher\n",
    "tag_to_ix = {'CU':0,\n",
    "             'FF':1,\n",
    "             'SL':2,\n",
    "             'CH':3} \n",
    "\n",
    "# -- Input Dimensions -- DO NOT CHANGE\n",
    "PREV_PITCH_DIM = 21\n",
    "NUM_PREV_PITCHES = 5 \n",
    "GAME_STATE_DIM = 15 \n",
    "\n",
    "# -- Hyperparameters -- DO CHANGE \n",
    "HIDDEN_DIM = 21\n",
    "OUT_DIM = 15\n",
    "GAME_OUT_DIM = 15\n",
    "batch_size = 50\n",
    "lstm_layers = 2\n",
    "learning_rate = 0.0001\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "    \n",
    "class PitchPredict(nn.Module):\n",
    "    def __init__(self, prev_pitch_dim, hidden_dim, num_prev_pitches,out_dim, game_state_dim, game_out_dim, num_ptypes):\n",
    "        super(PitchPredict, self).__init__()\n",
    "        #get constants\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.prev_pitch_dim = prev_pitch_dim\n",
    "        self.num_prev_pitches = num_prev_pitches\n",
    "        self.out_dim = out_dim\n",
    "        self.game_state_dim = game_state_dim\n",
    "        self.game_out_dim = game_out_dim\n",
    "        \n",
    "        ####Define Layers####\n",
    "        \n",
    "        ########################## LSTM for past five pitches#######################################\n",
    "        # The LSTM takes previous pitch vectors as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(prev_pitch_dim, hidden_dim,num_layers=lstm_layers,batch_first=True)#FIDDLE WITH num_layers\n",
    "\n",
    "        # The linear layer that maps from hidden state space to a vector\n",
    "        # with dimensionality OUT_DIM\n",
    "        self.hidden2out = nn.Linear(hidden_dim, out_dim)\n",
    "        \n",
    "        ############## FULLY CONNECTED LAYERS for LSTM OUTPUT + game_state vector #################\n",
    "        \n",
    "        # layer to map the game state vector to a different dimension\n",
    "        #self.l1 = nn.Linear(self.game_state_dim, self.game_out_dim)  \n",
    "        \n",
    "        # This Fully connected layer maps from the output of the final hidden layer output from the LSTM,\n",
    "        # dimension = OUT_DIM, with the game state vector, dimension = GAME_STATE_DIM\n",
    "        # to a vector of length of the number of ptypes to pass through softmax for probabilities\n",
    "        self.fc1 = nn.Linear((self.out_dim + self.game_out_dim), num_ptypes)\n",
    "\n",
    "                             \n",
    "    def forward(self, rep):\n",
    "        past_pitches,game_state = rep\n",
    "        lstm_out, _ = self.lstm(past_pitches.view(batch_size,self.num_prev_pitches, -1))\n",
    "        learned_rep = self.hidden2out(lstm_out.view(batch_size,self.num_prev_pitches, -1))\n",
    "        game_rep = game_state.view(batch_size,self.game_state_dim)\n",
    "        #game_rep = self.l1(game_state.view(batch_size,self.game_state_dim))\n",
    "        encoding = F.relu(learned_rep[:,self.num_prev_pitches - 1:,:])\n",
    "        fc_in = torch.cat((encoding.view(batch_size,self.out_dim),game_rep.view(batch_size,self.game_out_dim)),dim=1)\n",
    "        fc = self.fc1(fc_in.view(batch_size,self.game_out_dim+self.out_dim))\n",
    "        fc = F.relu(fc)\n",
    "        tag_scores = F.log_softmax(fc,dim=0)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************Pre-Training Accuracy*****************\n",
      "______________________________________\n",
      "curve rate: 0.17429438543247344\n",
      "Predicted curve rate: 0.3362670713201821\n",
      "______________________________________\n",
      "Fourseam Fastball rate: 0.5750834597875569\n",
      "Predicted Fourseam Fastball rate: 0.22355083459787556\n",
      "______________________________________\n",
      "Changeup rate: 0.1303793626707132\n",
      "Predicted changeup rate: 0.2755083459787557\n",
      "______________________________________\n",
      "slider rate: 0.12024279210925645\n",
      "Predicted slider rate: 0.16467374810318663\n",
      "______________________________________\n",
      "Accuracy: 0.24063732928679818\n",
      "Accuracy above naive guess: -0.3344461305007588\n",
      "______________________________________\n",
      "**************************Training*****************************\n",
      "epoch: 1 loss: 3.8987011909484863\n",
      "epoch: 2 loss: 3.8913588523864746\n",
      "epoch: 3 loss: 3.887312412261963\n",
      "epoch: 4 loss: 3.876291275024414\n",
      "epoch: 5 loss: 3.8633370399475098\n",
      "epoch: 6 loss: 3.8521759510040283\n",
      "epoch: 7 loss: 3.8406543731689453\n",
      "epoch: 8 loss: 3.8127527236938477\n",
      "epoch: 9 loss: 3.7970612049102783\n",
      "epoch: 10 loss: 3.786644220352173\n",
      "epoch: 11 loss: 3.7795302867889404\n",
      "epoch: 12 loss: 3.7737936973571777\n",
      "epoch: 13 loss: 3.769181489944458\n",
      "epoch: 14 loss: 3.765164852142334\n",
      "epoch: 15 loss: 3.7617321014404297\n",
      "epoch: 16 loss: 3.7587506771087646\n",
      "epoch: 17 loss: 3.7560293674468994\n",
      "epoch: 18 loss: 3.7535054683685303\n",
      "epoch: 19 loss: 3.751333236694336\n",
      "epoch: 20 loss: 3.7492504119873047\n",
      "epoch: 21 loss: 3.7472221851348877\n",
      "epoch: 22 loss: 3.745253086090088\n",
      "epoch: 23 loss: 3.743374824523926\n",
      "epoch: 24 loss: 3.741440534591675\n",
      "epoch: 25 loss: 3.739919424057007\n",
      "epoch: 26 loss: 3.7385599613189697\n",
      "epoch: 27 loss: 3.737074613571167\n",
      "epoch: 28 loss: 3.7354376316070557\n",
      "epoch: 29 loss: 3.7339510917663574\n",
      "epoch: 30 loss: 3.7325692176818848\n",
      "epoch: 31 loss: 3.7313506603240967\n",
      "epoch: 32 loss: 3.730195999145508\n",
      "epoch: 33 loss: 3.728994369506836\n",
      "epoch: 34 loss: 3.727682113647461\n",
      "epoch: 35 loss: 3.72639799118042\n",
      "epoch: 36 loss: 3.7249863147735596\n",
      "epoch: 37 loss: 3.723665237426758\n",
      "epoch: 38 loss: 3.722285747528076\n",
      "epoch: 39 loss: 3.7209832668304443\n",
      "epoch: 40 loss: 3.7198314666748047\n",
      "epoch: 41 loss: 3.718731641769409\n",
      "epoch: 42 loss: 3.71773099899292\n",
      "epoch: 43 loss: 3.7167630195617676\n",
      "epoch: 44 loss: 3.71567440032959\n",
      "epoch: 45 loss: 3.714674949645996\n",
      "epoch: 46 loss: 3.7137508392333984\n",
      "epoch: 47 loss: 3.712939739227295\n",
      "epoch: 48 loss: 3.7121992111206055\n",
      "epoch: 49 loss: 3.711489677429199\n",
      "epoch: 50 loss: 3.7111241817474365\n",
      "epoch: 51 loss: 3.7104995250701904\n",
      "epoch: 52 loss: 3.7099595069885254\n",
      "epoch: 53 loss: 3.709181547164917\n",
      "epoch: 54 loss: 3.708554744720459\n",
      "epoch: 55 loss: 3.7079668045043945\n",
      "epoch: 56 loss: 3.7076339721679688\n",
      "epoch: 57 loss: 3.706448554992676\n",
      "epoch: 58 loss: 3.7057764530181885\n",
      "epoch: 59 loss: 3.7053394317626953\n",
      "epoch: 60 loss: 3.704144239425659\n",
      "epoch: 61 loss: 3.703579902648926\n",
      "epoch: 62 loss: 3.703808069229126\n",
      "epoch: 63 loss: 3.702347993850708\n",
      "epoch: 64 loss: 3.7018446922302246\n",
      "epoch: 65 loss: 3.701672077178955\n",
      "epoch: 66 loss: 3.700669050216675\n",
      "epoch: 67 loss: 3.7002768516540527\n",
      "epoch: 68 loss: 3.6997628211975098\n",
      "epoch: 69 loss: 3.6993367671966553\n",
      "epoch: 70 loss: 3.6990973949432373\n",
      "epoch: 71 loss: 3.6985082626342773\n",
      "epoch: 72 loss: 3.6982128620147705\n",
      "epoch: 73 loss: 3.697899580001831\n",
      "epoch: 74 loss: 3.6972763538360596\n",
      "epoch: 75 loss: 3.6969046592712402\n",
      "epoch: 76 loss: 3.696423292160034\n",
      "epoch: 77 loss: 3.696072578430176\n",
      "epoch: 78 loss: 3.6957743167877197\n",
      "epoch: 79 loss: 3.695330858230591\n",
      "epoch: 80 loss: 3.6951847076416016\n",
      "epoch: 81 loss: 3.6947498321533203\n",
      "epoch: 82 loss: 3.694593906402588\n",
      "epoch: 83 loss: 3.6938817501068115\n",
      "epoch: 84 loss: 3.693678855895996\n",
      "epoch: 85 loss: 3.693443536758423\n",
      "epoch: 86 loss: 3.693113088607788\n",
      "epoch: 87 loss: 3.6928272247314453\n",
      "epoch: 88 loss: 3.6920387744903564\n",
      "epoch: 89 loss: 3.6918258666992188\n",
      "epoch: 90 loss: 3.6917924880981445\n",
      "epoch: 91 loss: 3.6916840076446533\n",
      "epoch: 92 loss: 3.691689968109131\n",
      "epoch: 93 loss: 3.691559076309204\n",
      "epoch: 94 loss: 3.6914236545562744\n",
      "epoch: 95 loss: 3.6912808418273926\n",
      "epoch: 96 loss: 3.6910324096679688\n",
      "epoch: 97 loss: 3.6909396648406982\n",
      "epoch: 98 loss: 3.6908676624298096\n",
      "epoch: 99 loss: 3.6906542778015137\n",
      "epoch: 100 loss: 3.6905288696289062\n",
      "epoch: 101 loss: 3.690410852432251\n",
      "epoch: 102 loss: 3.6902663707733154\n",
      "epoch: 103 loss: 3.690246820449829\n",
      "epoch: 104 loss: 3.69014835357666\n",
      "epoch: 105 loss: 3.6899240016937256\n",
      "epoch: 106 loss: 3.6896915435791016\n",
      "epoch: 107 loss: 3.689718723297119\n",
      "epoch: 108 loss: 3.6896562576293945\n",
      "epoch: 109 loss: 3.6896841526031494\n",
      "epoch: 110 loss: 3.6893999576568604\n",
      "epoch: 111 loss: 3.6893372535705566\n",
      "epoch: 112 loss: 3.6892812252044678\n",
      "epoch: 113 loss: 3.689424753189087\n",
      "epoch: 114 loss: 3.6891894340515137\n",
      "epoch: 115 loss: 3.6889939308166504\n",
      "epoch: 116 loss: 3.688863515853882\n",
      "epoch: 117 loss: 3.688741445541382\n",
      "epoch: 118 loss: 3.6885669231414795\n",
      "epoch: 119 loss: 3.688378095626831\n",
      "epoch: 120 loss: 3.688234567642212\n",
      "epoch: 121 loss: 3.688117265701294\n",
      "epoch: 122 loss: 3.687980890274048\n",
      "epoch: 123 loss: 3.6877667903900146\n",
      "epoch: 124 loss: 3.6875603199005127\n",
      "epoch: 125 loss: 3.6874279975891113\n",
      "epoch: 126 loss: 3.687347412109375\n",
      "epoch: 127 loss: 3.6871039867401123\n",
      "epoch: 128 loss: 3.6870272159576416\n",
      "epoch: 129 loss: 3.6875014305114746\n",
      "epoch: 130 loss: 3.6877968311309814\n",
      "epoch: 131 loss: 3.6872899532318115\n",
      "epoch: 132 loss: 3.6875510215759277\n",
      "epoch: 133 loss: 3.687748908996582\n",
      "epoch: 134 loss: 3.6879022121429443\n",
      "epoch: 135 loss: 3.687497615814209\n",
      "epoch: 136 loss: 3.687486171722412\n",
      "epoch: 137 loss: 3.687133550643921\n",
      "epoch: 138 loss: 3.6871416568756104\n",
      "epoch: 139 loss: 3.687007427215576\n",
      "epoch: 140 loss: 3.6870670318603516\n",
      "epoch: 141 loss: 3.6868534088134766\n",
      "epoch: 142 loss: 3.686711311340332\n",
      "epoch: 143 loss: 3.6865592002868652\n",
      "epoch: 144 loss: 3.686245918273926\n",
      "epoch: 145 loss: 3.686159610748291\n",
      "epoch: 146 loss: 3.686016798019409\n",
      "epoch: 147 loss: 3.6860599517822266\n",
      "epoch: 148 loss: 3.6860711574554443\n",
      "epoch: 149 loss: 3.6859312057495117\n",
      "epoch: 150 loss: 3.685931444168091\n",
      "epoch: 151 loss: 3.685838222503662\n",
      "epoch: 152 loss: 3.6860480308532715\n",
      "epoch: 153 loss: 3.685966730117798\n",
      "epoch: 154 loss: 3.685953378677368\n",
      "epoch: 155 loss: 3.6859328746795654\n",
      "epoch: 156 loss: 3.6860382556915283\n",
      "epoch: 157 loss: 3.6859512329101562\n",
      "epoch: 158 loss: 3.6858954429626465\n",
      "epoch: 159 loss: 3.6857895851135254\n",
      "epoch: 160 loss: 3.685911178588867\n",
      "epoch: 161 loss: 3.6859076023101807\n",
      "epoch: 162 loss: 3.685920476913452\n",
      "epoch: 163 loss: 3.6858603954315186\n",
      "epoch: 164 loss: 3.6858108043670654\n",
      "epoch: 165 loss: 3.685776710510254\n",
      "epoch: 166 loss: 3.6858110427856445\n",
      "epoch: 167 loss: 3.685736894607544\n",
      "epoch: 168 loss: 3.6857175827026367\n",
      "epoch: 169 loss: 3.6857171058654785\n",
      "epoch: 170 loss: 3.6855642795562744\n",
      "epoch: 171 loss: 3.6853768825531006\n",
      "epoch: 172 loss: 3.6855950355529785\n",
      "epoch: 173 loss: 3.68580961227417\n",
      "epoch: 174 loss: 3.686737060546875\n",
      "epoch: 175 loss: 3.686370849609375\n",
      "epoch: 176 loss: 3.6864397525787354\n",
      "epoch: 177 loss: 3.6863210201263428\n",
      "epoch: 178 loss: 3.6862103939056396\n",
      "epoch: 179 loss: 3.6851258277893066\n",
      "epoch: 180 loss: 3.68497371673584\n",
      "epoch: 181 loss: 3.6846468448638916\n",
      "epoch: 182 loss: 3.6847729682922363\n",
      "epoch: 183 loss: 3.684450149536133\n",
      "epoch: 184 loss: 3.6839263439178467\n",
      "epoch: 185 loss: 3.684126377105713\n",
      "epoch: 186 loss: 3.6840102672576904\n",
      "epoch: 187 loss: 3.6835291385650635\n",
      "epoch: 188 loss: 3.6834094524383545\n",
      "epoch: 189 loss: 3.682650089263916\n",
      "epoch: 190 loss: 3.681931734085083\n",
      "epoch: 191 loss: 3.682203769683838\n",
      "epoch: 192 loss: 3.68259334564209\n",
      "epoch: 193 loss: 3.681936025619507\n",
      "epoch: 194 loss: 3.682553768157959\n",
      "epoch: 195 loss: 3.6825900077819824\n",
      "epoch: 196 loss: 3.6823465824127197\n",
      "epoch: 197 loss: 3.682058095932007\n",
      "epoch: 198 loss: 3.6818466186523438\n",
      "epoch: 199 loss: 3.682131052017212\n",
      "epoch: 200 loss: 3.6820809841156006\n",
      "****************Post-Training Accuracy********************\n",
      "______________________________________\n",
      "curve rate: 0.17429438543247344\n",
      "Predicted curve rate: 0.23192716236722308\n",
      "______________________________________\n",
      "Fourseam Fastball rate: 0.5750834597875569\n",
      "Predicted Fourseam Fastball rate: 0.24100151745068285\n",
      "______________________________________\n",
      "Changeup rate: 0.1303793626707132\n",
      "Predicted changeup rate: 0.27195751138088015\n",
      "______________________________________\n",
      "slider rate: 0.12024279210925645\n",
      "Predicted slider rate: 0.255113808801214\n",
      "______________________________________\n",
      "Accuracy: 0.37969650986342945\n",
      "Accuracy above naive guess: -0.19538694992412747\n",
      "______________________________________\n",
      "**********************************************************\n"
     ]
    }
   ],
   "source": [
    "model = PitchPredict(PREV_PITCH_DIM, HIDDEN_DIM, NUM_PREV_PITCHES, OUT_DIM, GAME_STATE_DIM, GAME_OUT_DIM, len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)#FIDDLE WITH LEARNING RATE (lr)\n",
    "\n",
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    print(\"***************Pre-Training Accuracy*****************\")\n",
    "    test_accuracy(train_batches,model)\n",
    "print(\"**************************Training*****************************\")\n",
    "for epoch in range(200):\n",
    "    length = len(train_batches)*len(train_batches[0][0])\n",
    "    num_right = 0\n",
    "    ff_count = 0\n",
    "    predict_ff = 0\n",
    "    for batch in train_batches:\n",
    "        prev_pitches ,pre_pitch, ptypes = batch\n",
    "        \n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        #get input tensors ready\n",
    "        prevs_in = torch.tensor(prev_pitches, dtype=torch.float)\n",
    "        game_state_in = torch.tensor(pre_pitch, dtype=torch.float)\n",
    "        \n",
    "        #get target value\n",
    "        target = [ tag_to_ix[ptype] for ptype in ptypes]\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        if torch.cuda.is_available():\n",
    "            tag_scores = model((prevs_in,game_state_in)).cuda()\n",
    "        else:\n",
    "            tag_scores = model((prevs_in,game_state_in))\n",
    "               \n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores.view(batch_size,-1), torch.tensor(target,dtype=torch.long))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        preds = [tag_score.max(0) for tag_score in tag_scores]\n",
    "        #print(index.item())\n",
    "        for pred,target in zip(preds,targets):\n",
    "            _,index = pred\n",
    "            if target == 1:\n",
    "                ff_count += 1\n",
    "            if index.item() == 1:\n",
    "                predict_ff += 1\n",
    "            if index.item() == target:\n",
    "                num_right += 1   \n",
    "                \n",
    "    #display post-epoch results\n",
    "    ff_rate = ff_count/length\n",
    "    pred_ff_rate = predict_ff/length\n",
    "    accuracy = num_right/length\n",
    "    print('epoch:',epoch+1,\"loss:\",loss.item())\n",
    "\n",
    "# See what the scores are after training\n",
    "with torch.no_grad():\n",
    "    print(\"****************Post-Training Accuracy********************\")\n",
    "    test_accuracy(train_batches,model.eval())\n",
    "    print(\"**********************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************Test Accuracy********************\n",
      "______________________________________\n",
      "curve rate: 0.1596923076923077\n",
      "Predicted curve rate: 0.23476923076923076\n",
      "______________________________________\n",
      "Fourseam Fastball rate: 0.5864615384615385\n",
      "Predicted Fourseam Fastball rate: 0.23015384615384615\n",
      "______________________________________\n",
      "Changeup rate: 0.027692307692307693\n",
      "Predicted changeup rate: 0.27784615384615385\n",
      "______________________________________\n",
      "slider rate: 0.22615384615384615\n",
      "Predicted slider rate: 0.2572307692307692\n",
      "______________________________________\n",
      "Accuracy: 0.3252307692307692\n",
      "Accuracy above naive guess: -0.26123076923076927\n",
      "______________________________________\n",
      "**********************************************************\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(\"****************Test Accuracy********************\")\n",
    "    test_accuracy(test_batches,model)\n",
    "    print(\"**********************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### high score using balanced set (25% FF)\n",
    "- **TRAINING ACCURACY HIGH SCORE:** 38 (39% FF)\n",
    "\n",
    "- **TEST ACCURACY HIGH SCORE:** 43.7 (37% FF) -14%- naive \n",
    "\n",
    "\n",
    "- ****HYPERPARAMETERS:****\n",
    "\n",
    "    - LSTM num_layers: 1\n",
    "    \n",
    "    - OUT_SIZE: 15\n",
    "    \n",
    "    - HIDDEN_DIMENSION: 10\n",
    "    \n",
    "    - EPOCHS: 5\n",
    "   \n",
    "    - LEARNING RATE: 0.001\n",
    "    \n",
    "    - LOSS FUNCTION: NLLLOSS\n",
    "    \n",
    "    - OPTIMIZER: ADAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### high score using unbalanced set (50+% FF)\n",
    "- **TRAINING ACCURACY HIGH SCORE:** 57.8 (91% FF)\n",
    "\n",
    "- **TEST ACCURACY HIGH SCORE:** 60.86 (88% FF) 2.5%+ naive \n",
    "\n",
    "\n",
    "- ****HYPERPARAMETERS:****\n",
    "\n",
    "    - LSTM num_layers: 1\n",
    "    \n",
    "    - OUT_SIZE: 15\n",
    "    \n",
    "    - HIDDEN_DIMENSION: 10\n",
    "    \n",
    "    - EPOCHS: 20\n",
    "   \n",
    "    - LEARNING RATE: 0.001\n",
    "    \n",
    "    - LOSS FUNCTION: NLLLOSS\n",
    "    \n",
    "    - OPTIMIZER: ADAM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
