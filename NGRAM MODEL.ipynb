{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting pitches using the Pytorch N-Gram Language Modeling example (https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "verlander = pd.read_csv('Data/raw_data/verlander.csv')\n",
    "scherzer = pd.read_csv('Data/raw_data/scherzer.csv')\n",
    "lester =  pd.read_csv('Data/raw_data/lester.csv')\n",
    "hamels =  pd.read_csv('Data/raw_data/hamels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CU', 'FF', 'SL', 'CH', nan, 'FC', 'FT', 'IN', 'PO'], dtype=object)"
      ]
     },
     "execution_count": 718,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#different pitch types \n",
    "verlander['pitch_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get A list of at-bats. Each at-bat is a list of pitches that the pitcher threw to a specific batter\n",
    "def get_abs(pitcher):\n",
    "    pitcher_abs = []\n",
    "    ab_num = pitcher['at_bat_number']\n",
    "    pitch_types = pitcher['pitch_type']\n",
    "    ab_pitches = []\n",
    "    for index in reversed(range(len(pitch_types))):\n",
    "        if index == 0:\n",
    "            ab_pitches.append(pitch_types[index])\n",
    "        elif ab_num[index] != ab_num[index - 1]:\n",
    "            pitcher_abs.append(ab_pitches)\n",
    "            ab_pitches = [pitch_types[index]]\n",
    "        else:\n",
    "            ab_pitches.append(pitch_types[index])\n",
    "    return pitcher_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decide what pitcher to run the model with here\n",
    "ab_list = get_abs(verlander)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove any at-bat that has a NaN pitch type\n",
    "def clear_nan_abs(ab_list):\n",
    "    nan_abs = 0\n",
    "    for ab in ab_list:\n",
    "        is_na = False\n",
    "        for pitch in ab:\n",
    "            if pitch != pitch:\n",
    "                is_na = True\n",
    "                break\n",
    "        if is_na:\n",
    "            nan_abs += 1\n",
    "            ab_list.remove(ab)\n",
    "    return nan_abs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear nan columns\n",
    "x = 1\n",
    "while x != 0:\n",
    "    x = clear_nan_abs(ab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe54d95f350>"
      ]
     },
     "execution_count": 723,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get train and test sets\n",
    "train = ab_list[1:7856]\n",
    "test = ab_list[7856:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this creates the ngrams from the at-bats, with the context length as a parameter\n",
    "def create_ngrams(data,con_len):\n",
    "    ngrams = []\n",
    "    for ab in data:\n",
    "        ab = ([\"NA\"]*con_len) + ab\n",
    "        for i in range(len(ab) - con_len):\n",
    "              context = ab[i:i+con_len]\n",
    "              target = ab[i+con_len]\n",
    "              ngrams.append((context,target))                        \n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create ngrams for test and train\n",
    "ngrams_test = create_ngrams(test,5)\n",
    "ngrams_train = create_ngrams(train,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['FT', 'FT'], ['CH'], ['CH', 'FT', 'CH', 'FT', 'CH', 'CH']]"
      ]
     },
     "execution_count": 727,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example: displayed below are 3 at-bats\n",
    "train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['NA', 'NA', 'NA', 'NA', 'NA'], 'FT'),\n",
       " (['NA', 'NA', 'NA', 'NA', 'FT'], 'FT'),\n",
       " (['NA', 'NA', 'NA', 'NA', 'NA'], 'CH'),\n",
       " (['NA', 'NA', 'NA', 'NA', 'NA'], 'CH'),\n",
       " (['NA', 'NA', 'NA', 'NA', 'CH'], 'FT'),\n",
       " (['NA', 'NA', 'NA', 'CH', 'FT'], 'CH'),\n",
       " (['NA', 'NA', 'CH', 'FT', 'CH'], 'FT'),\n",
       " (['NA', 'CH', 'FT', 'CH', 'FT'], 'CH'),\n",
       " (['CH', 'FT', 'CH', 'FT', 'CH'], 'CH')]"
      ]
     },
     "execution_count": 728,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#here is the translation of each of those pitches to an ngram. The first pitch of the at-bat is padded with 'NA' values,\n",
    "# and the padding continues until the number of prior-pitches in the at-bat equals the context size \n",
    "ngrams_train[:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function gets back the rate of the most frequent pitch (fastball in nearly every case), which will serve as the \n",
    "#benchmark\n",
    "def get_benchmark(test):\n",
    "    benchmark = 0\n",
    "    for i in range(len(test)):\n",
    "        if test[i][1] == 'FF':\n",
    "            benchmark += 1\n",
    "    return benchmark/len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.586978255436141"
      ]
     },
     "execution_count": 730,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#must beat this mark\n",
    "get_benchmark(ngrams_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss: 1.3114299774169922\n",
      "epoch: 2 loss: 1.228050947189331\n",
      "epoch: 3 loss: 1.2007803916931152\n",
      "epoch: 4 loss: 1.1942858695983887\n",
      "epoch: 5 loss: 1.1964590549468994\n"
     ]
    }
   ],
   "source": [
    "#model I copied from pytorch website lol\n",
    "CONTEXT_SIZE = 5\n",
    "EMBEDDING_DIM = 10\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "\n",
    "vocab = set(['CU', 'FF', 'SL', 'CH', 'NA', 'FC', 'FT', 'IN', 'PO'])\n",
    "pitch_to_ix = {pitch: i for i, pitch in enumerate(vocab)}\n",
    "ix_to_pitch = {i: pitch for i, pitch in enumerate(vocab)}\n",
    "\n",
    "\n",
    "class NGramModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, 64)\n",
    "        self.linear3 = nn.Linear(64, vocab_size)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.linear3(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramModel(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(5):\n",
    "    total_loss = 0\n",
    "    for context, target in ngrams_train:\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in tensors)\n",
    "        context_idxs = torch.tensor([pitch_to_ix[p] for p in context], dtype=torch.long)\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_idxs).cuda()\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "        loss = loss_function(log_probs, torch.tensor([pitch_to_ix[target]], dtype=torch.long))\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "    print(\"epoch:\",epoch+1,\"loss:\",loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy above naive guess: -0.0004998750312422695 %\n"
     ]
    }
   ],
   "source": [
    "#calculate accuracy\n",
    "correct = 0\n",
    "for context,pitch in ngrams_test:\n",
    "    values, idx = model(torch.tensor([pitch_to_ix[p] for p in context], dtype=torch.long))[0].max(0)\n",
    "    #if ix_to_pitch[idx.item()] != 'FF':\n",
    "        #print(ix_to_pitch[idx.item()])\n",
    "    if ix_to_pitch[idx.item()] == pitch:\n",
    "        correct += 1\n",
    "print(\"accuracy above naive guess:\",(correct/len(ngrams_test))-get_benchmark(ngrams_test),\"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
